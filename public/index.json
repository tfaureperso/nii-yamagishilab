
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":"Isao Echizen","categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1672500190,"objectID":"582b21f39122efdc484b330c43a24af0","permalink":"https://zlin0.github.io/nii-yamagishilab/author/isao-echizen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/isao-echizen/","section":"authors","summary":"","tags":null,"title":"Isao Echizen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0cb0a909489ffac2838902700867d77e","permalink":"https://zlin0.github.io/nii-yamagishilab/author/yamagishis-lab/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/yamagishis-lab/","section":"authors","summary":"","tags":null,"title":"Yamagishi's Lab","type":"authors"},{"authors":"Junichi Yamagishi","categories":null,"content":"Junichi Yamagishi received the Ph.D. degree from Tokyo Institute of Technology in 2006 for a thesis that pioneered speaker-adaptive speech synthesis. He is currently a Professor with the National Institute of Informatics, Tokyo, Japan, and also a Senior Research Fellow with the Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.K. Since 2006, he has authored and co-authored more than 250 refereed papers in international journals and conferences.\nHe was an area coordinator at Interspeech 2012. He was one of organizers for special sessions on “Spoofing and Countermeasures for Automatic Speaker Verification” at Interspeech 2013, “ASVspoof evaluation” at Interspeech 2015, “Voice conversion challenge 2016” at Interspeech 2016, “2nd ASVspoof evaluation” at Interspeech 2017, and “Voice conversion challenge 2018” at Speaker Odyssey 2018. He is currently an organizing committee for ASVspoof 2019, an organizing committee for ISCA the 10th ISCA Speech Synthesis Workshop 2019, a technical program committee for IEEE ASRU 2019, and an award committee for ISCA Speaker Odyssey 2020.\nHe was a member of IEEE Speech and Language Technical Committee. He was also an Associate Editor of the IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING and a Lead Guest Editor for the IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING special issue on Spoofing and Countermeasures for Automatic Speaker Verification. He is currently a guest editor for Computer Speech and Language special issue on speaker and language characterization and recognition: voice modeling, conversion, synthesis and ethical aspects. He also serves as a chairperson of ISCA SynSIG currently.\nHe was the recipient of the Tejima Prize as the best Ph.D. thesis of Tokyo Institute of Technology in 2007. He received the Itakura Prize from the Acoustic Society of Japan in 2010, the Kiyasu Special Industrial Achievement Award from the Information Processing Society of Japan in 2013, the Young Scientists’ Prize from the Minister of Education, Science and Technology in 2014, the JSPS Prize from Japan Society for the Promotion of Science in 2016, and Docomo mobile science award from Mobile communication fund in 2018.\n","date":1676678400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676819037,"objectID":"fdbbe40a945160dd41e945575d9ac121","permalink":"https://zlin0.github.io/nii-yamagishilab/author/junichi-yamagishi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/junichi-yamagishi/","section":"authors","summary":"Junichi Yamagishi received the Ph.D. degree from Tokyo Institute of Technology in 2006 for a thesis that pioneered speaker-adaptive speech synthesis. He is currently a Professor with the National Institute of Informatics, Tokyo, Japan, and also a Senior Research Fellow with the Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.","tags":null,"title":"Junichi Yamagishi","type":"authors"},{"authors":"Xin Wang","categories":null,"content":"Xin Wang is a Project Researcher at the National Institute of Informatics, Japan. He received the Ph.D. degree from SOKENDAI, Japan, in 2018. Before that, he received M.S. and B.E degrees from the University of Science and Technology of China and University of Electronic Science and Technology of China in 2015 and 2012, respectively. His research interests include statistical speech synthesis and machine learning.\n","date":1676678400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676819037,"objectID":"60431f41a7738642124d36689545ee9a","permalink":"https://zlin0.github.io/nii-yamagishilab/author/xin-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/xin-wang/","section":"authors","summary":"Xin Wang is a Project Researcher at the National Institute of Informatics, Japan. He received the Ph.D. degree from SOKENDAI, Japan, in 2018. Before that, he received M.S. and B.E degrees from the University of Science and Technology of China and University of Electronic Science and Technology of China in 2015 and 2012, respectively.","tags":null,"title":"Xin Wang","type":"authors"},{"authors":"Erica Cooper","categories":null,"content":"Erica Cooper received a B.Sc. degree and M.Eng. degree both in electrical engineering and computer science from the Massachusetts Institute of Technology, Cambridge, MA, USA, in 2009 and 2010, respectively. She received a Ph.D. degree in computer science from Columbia University, New York, NY, USA, in 2019. Since 2019, she has been a Project Researcher with the National Institute of Informatics, Chiyoda, Tokyo, Japan. Her research interests include statistical machine learning and speech synthesis. Dr. Cooper’s awards include the 3rd Prize in the CSAW Voice Biometrics and Speech Synthesis Competition, the Computer Science Service Award from Columbia University, and the Best Poster Award in the Speech Processing Courses in Crete.\n","date":1676678400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676699024,"objectID":"d8454c46448c7223d01580e85099c60d","permalink":"https://zlin0.github.io/nii-yamagishilab/author/erica-cooper/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/erica-cooper/","section":"authors","summary":"Erica Cooper received a B.Sc. degree and M.Eng. degree both in electrical engineering and computer science from the Massachusetts Institute of Technology, Cambridge, MA, USA, in 2009 and 2010, respectively. She received a Ph.","tags":null,"title":"Erica Cooper","type":"authors"},{"authors":"Canasai Kruengkrai","categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676819037,"objectID":"9b2b3c0d0409027d2a9be8ab4e57ca77","permalink":"https://zlin0.github.io/nii-yamagishilab/author/canasai-kruengkrai/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/canasai-kruengkrai/","section":"authors","summary":"","tags":null,"title":"Canasai Kruengkrai","type":"authors"},{"authors":"Hieu-Thi Luong","categories":null,"content":"I received my Ph.D. degree in Multidisciplinary Science in 2020 from SOKENDAI, Japan, and currently work as Project Researcher at National Institute of Informatics, Tokyo. My works focus on researching and developing novel solutions for Speech and Language Processing Systems. I’m interested in Speech Processing, Machine Learning and Natural Language Processing in general.\nI also do programming and drawing as hobbies. More below.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5ab884c450c302340e46a95608fc966d","permalink":"https://zlin0.github.io/nii-yamagishilab/author/hieu-thi-luong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/hieu-thi-luong/","section":"authors","summary":"I received my Ph.D. degree in Multidisciplinary Science in 2020 from SOKENDAI, Japan, and currently work as Project Researcher at National Institute of Informatics, Tokyo. My works focus on researching and developing novel solutions for Speech and Language Processing Systems.","tags":null,"title":"Hieu-Thi Luong","type":"authors"},{"authors":"XiaoXiao Miao","categories":null,"content":"","date":1676678400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676699033,"objectID":"6501e27f5543f3b1589f38bee6fd8924","permalink":"https://zlin0.github.io/nii-yamagishilab/author/xiaoxiao-miao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/xiaoxiao-miao/","section":"authors","summary":"","tags":null,"title":"XiaoXiao Miao","type":"authors"},{"authors":null,"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1668948781,"objectID":"0130e056704c421a711e5bb33733d8ed","permalink":"https://zlin0.github.io/nii-yamagishilab/author/yusuke-yasuda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/yusuke-yasuda/","section":"authors","summary":"","tags":null,"title":"Yusuke Yasuda","type":"authors"},{"authors":"Huy Hong Nguyen","categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1672500190,"objectID":"21004a495ae8232538d535debb44a64a","permalink":"https://zlin0.github.io/nii-yamagishilab/author/huy-hong-nguyen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/huy-hong-nguyen/","section":"authors","summary":"","tags":null,"title":"Huy Hong Nguyen","type":"authors"},{"authors":"Haoyu Li","categories":null,"content":"","date":1676678400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676724768,"objectID":"db8c4a209c47cd6c85757c77a85fa51d","permalink":"https://zlin0.github.io/nii-yamagishilab/author/haoyu-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/haoyu-li/","section":"authors","summary":"","tags":null,"title":"Haoyu Li","type":"authors"},{"authors":"Chang Zeng","categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1668948774,"objectID":"144e4dc0a52b6d1ad0ce12e165fb8b57","permalink":"https://zlin0.github.io/nii-yamagishilab/author/chang-zeng/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/chang-zeng/","section":"authors","summary":"","tags":null,"title":"Chang Zeng","type":"authors"},{"authors":["lin-zhang"],"categories":null,"content":"","date":1672531200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1675262697,"objectID":"96084a286f91b5025e7ef274b43455a1","permalink":"https://zlin0.github.io/nii-yamagishilab/author/lin-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/lin-zhang/","section":"authors","summary":"","tags":null,"title":"Lin Zhang","type":"authors"},{"authors":"Xuan Shi","categories":null,"content":"","date":1676678400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676699024,"objectID":"84d63b784529ee3ee3a26585a3c89542","permalink":"https://zlin0.github.io/nii-yamagishilab/author/xuan-shi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/xuan-shi/","section":"authors","summary":"","tags":null,"title":"Xuan Shi","type":"authors"},{"authors":"Yun Liu","categories":null,"content":"","date":1676678400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676724768,"objectID":"1c67f3e29473721acea39fb3721b668e","permalink":"https://zlin0.github.io/nii-yamagishilab/author/yun-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/yun-liu/","section":"authors","summary":"","tags":null,"title":"Yun Liu","type":"authors"},{"authors":"Yi-chen Chang","categories":null,"content":"","date":1677636000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1677636000,"objectID":"2eef1156111a7972e9f36f09bcf0a2a9","permalink":"https://zlin0.github.io/nii-yamagishilab/author/yi-chen-chang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/yi-chen-chang/","section":"authors","summary":"","tags":null,"title":"Yi-chen Chang","type":"authors"},{"authors":"Nicolas Jonason","categories":null,"content":"","date":1675929600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1675929600,"objectID":"985f0ec2512dca004668496d01650e9c","permalink":"https://zlin0.github.io/nii-yamagishilab/author/nicolas-jonason/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/nicolas-jonason/","section":"authors","summary":"","tags":null,"title":"Nicolas Jonason","type":"authors"},{"authors":"Cheng Gong","categories":null,"content":"","date":1675756800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1675756800,"objectID":"f7c0024a474adaa63372f318eba2f7fc","permalink":"https://zlin0.github.io/nii-yamagishilab/author/cheng-gong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/cheng-gong/","section":"authors","summary":"","tags":null,"title":"Cheng Gong","type":"authors"},{"authors":"Li-Kuang Chen","categories":null,"content":"","date":1667260800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1676801193,"objectID":"80b258337321422dd534e521f855e9dc","permalink":"https://zlin0.github.io/nii-yamagishilab/author/li-kuang-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/li-kuang-chen/","section":"authors","summary":"","tags":null,"title":"Li-Kuang Chen","type":"authors"},{"authors":"Lifan Zhong","categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7a5b6bb98b943e6174e5b4873c9446c7","permalink":"https://zlin0.github.io/nii-yamagishilab/author/lifan-zhong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/lifan-zhong/","section":"authors","summary":"","tags":null,"title":"Lifan Zhong","type":"authors"},{"authors":"Yi Zhao","categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1668948781,"objectID":"fd7d59cb1d33da1fbab0449a0e306bbc","permalink":"https://zlin0.github.io/nii-yamagishilab/author/yi-zhao/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/yi-zhao/","section":"authors","summary":"","tags":null,"title":"Yi Zhao","type":"authors"},{"authors":"Shuhei Kato","categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1668948781,"objectID":"173291836a72b51e789e45f94b86b10e","permalink":"https://zlin0.github.io/nii-yamagishilab/author/shuhei-kato/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/shuhei-kato/","section":"authors","summary":"","tags":null,"title":"Shuhei Kato","type":"authors"},{"authors":"Fuming Fang","categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9754051adf906143e631493ee16c7633","permalink":"https://zlin0.github.io/nii-yamagishilab/author/fuming-fang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/fuming-fang/","section":"authors","summary":"","tags":null,"title":"Fuming Fang","type":"authors"},{"authors":"Shinji Takaki","categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3b4973d1a2a75792bea93efac1ead917","permalink":"https://zlin0.github.io/nii-yamagishilab/author/shinji-takaki/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/shinji-takaki/","section":"authors","summary":"","tags":null,"title":"Shinji Takaki","type":"authors"},{"authors":"Gustav Eje Henter","categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f5640479769e91c1c08e2aca4830274d","permalink":"https://zlin0.github.io/nii-yamagishilab/author/gustav-eje-henter/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/gustav-eje-henter/","section":"authors","summary":"","tags":null,"title":"Gustav Eje Henter","type":"authors"},{"authors":"Jaime Lorenzo-Trueba","categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d639d60f1e79461bed3a8141f09a9673","permalink":"https://zlin0.github.io/nii-yamagishilab/author/jaime-lorenzo-trueba/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/jaime-lorenzo-trueba/","section":"authors","summary":"","tags":null,"title":"Jaime Lorenzo-Trueba","type":"authors"},{"authors":"Fernando Villavicencio","categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5871ca992cc8cccf433ea858901854c0","permalink":"https://zlin0.github.io/nii-yamagishilab/author/fernando-villavicencio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/fernando-villavicencio/","section":"authors","summary":"","tags":null,"title":"Fernando Villavicencio","type":"authors"},{"authors":"Kaori Takaki","categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6119cd298c8b98a32a0dafc0f68a6f0a","permalink":"https://zlin0.github.io/nii-yamagishilab/author/kaori-takaki/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/author/kaori-takaki/","section":"authors","summary":"","tags":null,"title":"Kaori Takaki","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://zlin0.github.io/nii-yamagishilab/event/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/event/example/","section":"event","summary":"An example event.","tags":[],"title":"Example Event","type":"event"},{"authors":["Yi-chen Chang"],"categories":null,"content":"","date":1677636000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677636000,"objectID":"61e4ecf97bc338f5319329d672391547","permalink":"https://zlin0.github.io/nii-yamagishilab/talks/upcoming/2023-03-01-yc/","publishdate":"2023-02-19T19:43:32+09:00","relpermalink":"/nii-yamagishilab/talks/upcoming/2023-03-01-yc/","section":"talks","summary":"TBA","tags":[],"title":"TBA","type":"talks"},{"authors":["Xuan Shi","Erica Cooper","Xin Wang","Junichi Yamagishi","Shrikanth Narayanan"],"categories":[],"content":"","date":1676678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676699024,"objectID":"6c8ed79a6b673b7cea3ad70c93b7029c","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/shi-2022-can/","publishdate":"2023-02-18T05:43:43.870261Z","relpermalink":"/nii-yamagishilab/publication/shi-2022-can/","section":"publication","summary":"","tags":[],"title":"Can Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems?","type":"publication"},{"authors":["Paul-Gauthier Noé","XiaoXiao Miao","Xin Wang","Junichi Yamagishi","Jean-François Bonastre","Driss Matrouf"],"categories":[],"content":"","date":1676678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676699033,"objectID":"5b9cc4c2f4fc5ba69e5bf8e6a282fb33","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/noe-2022-hiding/","publishdate":"2022-12-08T05:43:53.42705Z","relpermalink":"/nii-yamagishilab/publication/noe-2022-hiding/","section":"publication","summary":"","tags":[],"title":"Hiding speaker's sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline","type":"publication"},{"authors":["Haoyu Li","Yun Liu","Junichi Yamagishi"],"categories":[],"content":"","date":1676678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676724768,"objectID":"a710855db33808e13e8bb2dd2035a213","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/li-2022-joint/","publishdate":"2023-02-18T12:52:48.455585Z","relpermalink":"/nii-yamagishilab/publication/li-2022-joint/","section":"publication","summary":"","tags":[],"title":"Joint Noise Reduction and Listening Enhancement for Full-End Speech Enhancement","type":"publication"},{"authors":["Xin Wang","Junichi Yamagishi"],"categories":[],"content":"","date":1676678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676678400,"objectID":"516a858002d9b06c8efa3e0e3abdba88","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/wang-2022-spoofed/","publishdate":"2023-02-18T12:52:47.375417Z","relpermalink":"/nii-yamagishilab/publication/wang-2022-spoofed/","section":"publication","summary":"","tags":[],"title":"Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders","type":"publication"},{"authors":[],"categories":[],"content":"We have four papers were accepted to ICASSP2023:\nPaul-Gauthier Noé, XiaoXiao Miao, Xin Wang, Junichi Yamagishi, Jean-François Bonastre, Driss Matrouf(2022). Hiding speaker’s sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline Xuan Shi, Erica Cooper, Xin Wang, Junichi Yamagishi, Shrikanth Narayanan (2022). Can Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems? Xin Wang, Junichi Yamagishi (2022). Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders Haoyu Li, Yun Liu, Junichi Yamagishi (2022). Joint Noise Reduction and Listening Enhancement for Full-End Speech Enhancement ","date":1676505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676730593,"objectID":"03d723b2b275d161dc52a10f8278dfc7","permalink":"https://zlin0.github.io/nii-yamagishilab/news/latest/2023-02-17-icassp/","publishdate":"2023-02-16T00:00:00Z","relpermalink":"/nii-yamagishilab/news/latest/2023-02-17-icassp/","section":"news","summary":"We have four papers were accepted to ICASSP2023:\nPaul-Gauthier Noé, XiaoXiao Miao, Xin Wang, Junichi Yamagishi, Jean-François Bonastre, Driss Matrouf(2022). Hiding speaker’s sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline Xuan Shi, Erica Cooper, Xin Wang, Junichi Yamagishi, Shrikanth Narayanan (2022).","tags":["Accepted papers"],"title":"Four papers were accepted to ICASSP2023!","type":"news"},{"authors":["Nicolas Jonason"],"categories":null,"content":"","date":1675929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675929600,"objectID":"da5ea84b9de46867ec278f7df327e4eb","permalink":"https://zlin0.github.io/nii-yamagishilab/talks/past/2023-02-09-nicolas/","publishdate":"2023-02-05T19:46:42+09:00","relpermalink":"/nii-yamagishilab/talks/past/2023-02-09-nicolas/","section":"talks","summary":" In my talk, I will share my research on enabling Human-AI co-creativity in music making. I will start by discussing my previous work on MIDI-to-audio synthesis using Control-Synthesis. Then, I will delve into my work on few-shot neural instrument cloning with multi-instrument models. I will also present my recent work on TimbreCLIP, a text/audio embedding I trained on instrument notes. Additionally, I will showcase some of my work on generating symbolic music. Finally, I will give a brief overview of the work I will be pursuing at NII.","tags":[],"title":"Enabling Human-AI Co-Creativity in Music Making","type":"talks"},{"authors":["Cheng Gong"],"categories":null,"content":"","date":1675756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675756800,"objectID":"bb414e8869284ec356cc994e696f88da","permalink":"https://zlin0.github.io/nii-yamagishilab/talks/past/2023-02-07-gong/","publishdate":"2023-02-05T19:46:28+09:00","relpermalink":"/nii-yamagishilab/talks/past/2023-02-07-gong/","section":"talks","summary":"I will give self-introduction and have a briefly introduction of some related research on speech synthesis in our Tianjin University laboratory at first. Next, I will talk about my Ph.d topic which is expressive speech synthesis and will mainly introduce some of my previous research. Then, I will simply introduce the topic \"Cross-speaker Style Transfer \" and talk about my recently work and the big challenge \"Few-shot\" that is what I want to solve Next. Finally, I will discuss another topic I will be involved in at NII, speech synthesis in low resource languages. In my opinion, low resources could also be considered as a kind of Few-shot problem, which is also exactly related to my current PhD research.","tags":["synthesis"],"title":"Few-shot Cross-speaker Style Transfer in Expressive Speech Synthesis","type":"talks"},{"authors":["Xin Wang","Junichi Yamagishi"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676797134,"objectID":"518c8df2bfa7e4ed6774e758bb08f2ea","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/wang-2022-investigating/","publishdate":"2023-02-19T10:06:09.768894Z","relpermalink":"/nii-yamagishilab/publication/wang-2022-investigating/","section":"publication","summary":"","tags":[],"title":"Investigating Active-Learning-Based Training Data Selection for Speech Spoofing Countermeasure","type":"publication"},{"authors":["Lin Zhang","Xin Wang","Erica Cooper","Nicholas Evans","Junichi Yamagishi"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1675262697,"objectID":"ff6838dd433ee58c827dd66f6339f45a","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/zhang-2022-partial-spoof/","publishdate":"2023-02-01T14:44:57.000799Z","relpermalink":"/nii-yamagishilab/publication/zhang-2022-partial-spoof/","section":"publication","summary":"Automatic speaker verification is susceptible to various manipulations and spoofing, such as text-to-speech synthesis, voice conversion, replay, tampering, adversarial attacks, and so on. We consider a new spoofing scenario called “Partial Spoof” (PS) in which synthesized or transformed speech segments are embedded into a bona fide utterance. While existing countermeasures (CMs) can detect fully spoofed utterances, there is a need for their adaptation or extension to the PS scenario. We propose various improvements to construct a significantly more accurate CM that can detect and locate short-generated spoofed speech segments at finer temporal resolutions. First, we introduce newly developed self-supervised pre-trained models as enhanced feature extractors. Second, we extend our PartialSpoof database by adding segment labels for various temporal resolutions. Since the short spoofed speech segments to be embedded by attackers are of variable length, six different temporal resolutions are considered, ranging from as short as 20 ms to as large as 640 ms. Third, we propose a new CM that enables the simultaneous use of the segment-level labels at different temporal resolutions as well as utterance-level labels to execute utterance- and segment-level detection at the same time. We also show that the proposed CM is capable of detecting spoofing at the utterance level with low error rates in the PS scenario as well as in a related logical access (LA) scenario. The equal error rates of utterance-level detection on the PartialSpoof database and ASVspoof 2019 LA database were 0.77 and 0.90%, respectively.","tags":[],"title":"The PartialSpoof Database and Countermeasures for the Detection of Short Fake Speech Segments Embedded in an Utterance","type":"publication"},{"authors":["Cheng-I Jeff Lai"],"categories":null,"content":"","date":1671548400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671548400,"objectID":"fd87e7c2fcc6d48fb8f053d8889cd143","permalink":"https://zlin0.github.io/nii-yamagishilab/talks/past/2022-12-20-jeff/","publishdate":"2022-12-18T00:00:00Z","relpermalink":"/nii-yamagishilab/talks/past/2022-12-20-jeff/","section":"talks","summary":"Textless Phrase-Structure Induction from Visually-Grounded Speech","tags":[],"title":"Textless Phrase-Structure Induction from Visually-Grounded Speech","type":"talks"},{"authors":["Bence Halpern"],"categories":null,"content":"","date":1671544800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671544800,"objectID":"d67ba3d19d9b624808cc64869af1ac92","permalink":"https://zlin0.github.io/nii-yamagishilab/talks/past/2022-12-21-bh/","publishdate":"2022-12-19T00:00:00Z","relpermalink":"/nii-yamagishilab/talks/past/2022-12-21-bh/","section":"talks","summary":"Predicting and synthesising plausible speech examples after oral cancer treatment","tags":[],"title":"Predicting and synthesising plausible speech examples after oral cancer treatment","type":"talks"},{"authors":["Li-Kuang Chen","Canasai Kruengkrai","Junichi Yamagishi"],"categories":[],"content":"","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676801193,"objectID":"88e6b553c2d10a87c396b2cdc31d2429","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/chen-2022-outlier/","publishdate":"2023-02-19T10:06:33.10273Z","relpermalink":"/nii-yamagishilab/publication/chen-2022-outlier/","section":"publication","summary":"Methods addressing spurious correlations such as Just Train Twice (JTT, Liu et al. 2021) involve reweighting a subset of the training set to maximize the worst-group accuracy. However, the reweighted set of examples may potentially contain unlearnable examples that hamper the model′s learning. We propose mitigating this by detecting outliers to the training set and removing them before reweighting. Our experiments show that our method achieves competitive or better accuracy compared with JTT and can detect and remove annotation errors in the subset being reweighted in JTT.","tags":[],"title":"Outlier-Aware Training for Improving Group Accuracy Disparities","type":"publication"},{"authors":[],"categories":[],"content":"We are recruiting postdoctoral researchers in speech and audio processing! https://www.nii.ac.jp/en/about/recruit/2022/1007-2.html\nWe are also recruiting postdocs for topics in media processing, machine learning, multimedia security forensics, and social media! https://www.nii.ac.jp/en/about/recruit/2022/1028.html\n","date":1667260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668868193,"objectID":"455601a2e2fd13e5525d8fac0ff337ea","permalink":"https://zlin0.github.io/nii-yamagishilab/news/old/2022-11-02-recruite/","publishdate":"2022-11-01T00:00:00Z","relpermalink":"/nii-yamagishilab/news/old/2022-11-02-recruite/","section":"news","summary":"We are recruiting postdoctoral researchers in speech and audio processing! https://www.nii.ac.jp/en/about/recruit/2022/1007-2.html\nWe are also recruiting postdocs for topics in media processing, machine learning, multimedia security forensics, and social media! https://www.nii.ac.jp/en/about/recruit/2022/1028.html","tags":["Jobs"],"title":"We are recruiting postdocs!","type":"news"},{"authors":["Xin Wang","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672500190,"objectID":"1019765ae371faa5cd87c6d27a796618","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/wang-2022/","publishdate":"2022-12-31T15:23:10.536358Z","relpermalink":"/nii-yamagishilab/publication/wang-2022/","section":"publication","summary":"Voice-based human-machine interfaces with an automatic speaker verification (ASV) component are commonly used in the market. However, the threat from presentation attacks is also growing since attackers can use recent speech synthesis technology to produce a natural-sounding voice of a victim. Presentation attack detection (PAD) for ASV, or speech anti-spoofing, is therefore indispensable. Research on voice PAD has seen significant progress since the early 2010s, including the advancement in PAD models, benchmark datasets, and evaluation campaigns. This chapter presents a practical guide to the field of voice PAD, with a focus on logical access attacks using text-to-speech and voice conversion algorithms and spoofing countermeasures based on artifact detection. It introduces the basic concept of voice PAD, explains the common techniques, and provides an experimental study using recent methods on a benchmark dataset. Code for the experiments is open-sourced.","tags":[],"title":"A Practical Guide to Logical Access Voice Presentation Attack Detection","type":"publication"},{"authors":["XiaoXiao Miao","Xin Wang","Erica Cooper","Junichi Yamagishi","Natalia Tomashenko"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948770,"objectID":"441dd99c91daae0bffe33b50525a36ec","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/miao-22-interspeech/","publishdate":"2022-11-20T12:52:49.893894Z","relpermalink":"/nii-yamagishilab/publication/miao-22-interspeech/","section":"publication","summary":"","tags":[],"title":"Analyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions","type":"publication"},{"authors":null,"categories":null,"content":"ASVspoof Latest Information Upcoming ASVspoof5 in 2023:\nDatabase creation: Autumn, 2022 Challenge set-up: First half, 2023 ASVspoof5 challenge: Second half, 2023 ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"7e7bfa7d312eb281d40aebbd80ffe513","permalink":"https://zlin0.github.io/nii-yamagishilab/challenges/asvspoof/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/challenges/asvspoof/","section":"challenges","summary":"The ASVspoof initiative was conceived to foster progress in the development of countermeasures (CM) to protect automatic speaker verification (ASV) systems from spoofing attacks.","tags":["ASVspoof"],"title":"ASVspoof","type":"challenges"},{"authors":["Chang Zeng","Xin Wang","Erica Cooper","XiaoXiao Miao","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948774,"objectID":"354f8d9da69032dc3264cef62616b868","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9746688/","publishdate":"2022-11-20T12:52:54.592096Z","relpermalink":"/nii-yamagishilab/publication/9746688/","section":"publication","summary":"","tags":[],"title":"Attention Back-End for Automatic Speaker Verification with Multiple Enrollment Utterances","type":"publication"},{"authors":["Hemlata Tak","Massimiliano Todisco","Xin Wang","Jee-weon Jung","Junichi Yamagishi","Nicholas Evans"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948771,"objectID":"3b3d786421e94ea0595069360d168c07","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/tak-22-odyssey/","publishdate":"2022-11-20T12:52:51.691202Z","relpermalink":"/nii-yamagishilab/publication/tak-22-odyssey/","section":"publication","summary":"","tags":[],"title":"Automatic Speaker Verification Spoofing and Deepfake Detection Using Wav2vec 2.0 and Data Augmentation","type":"publication"},{"authors":["Huy Hong Nguyen","Junichi Yamagishi","Isao Echizen"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672500188,"objectID":"5d083fb24ab841f4a45e3418a1141186","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/nguyen-2022/","publishdate":"2022-12-31T15:23:08.51697Z","relpermalink":"/nii-yamagishilab/publication/nguyen-2022/","section":"publication","summary":"SeveralCapsule-Forensics sophisticated convolutional neural network (CNN)Convolutional Neural Networks (CNN) architectures have been devised that have achieved impressive results in various domains. One downside of this success is the advent of attacks using deepfakesDeepFake, a family of tools that enable anyone to use a personal computer to easily create fake videos of someone from a short video found online. Several detectors have been introduced to deal with such attacks. To achieve state-of-the-art performance, CNN-basedConvolutional Neural Networks (CNN) detectors have usually been upgraded by increasing their depth and/or their width, adding more internal connections, or fusing several features or predicted probabilities from multiple CNNsConvolutional Neural Networks (CNN). As a result, CNN-basedConvolutional Neural Networks (CNN) detectors have become bigger, consume more memory and computation power, and require more training data. Moreover, there is concern about their generalizability to deal with unseen manipulation methods. In this chapter, we argue that our forensic-oriented capsule networkCapsule network overcomes these limitations and is more suitable than conventional CNNsConvolutional Neural Networks (CNN) to detect deepfakesDeepFake. The superiority of our ``Capsule-Forensics'' Capsule-Forensics network is due to the use of a pretrained feature extractor, statistical pooling layers, and a dynamic routing algorithm. This design enables the Capsule-ForensicsCapsule-Forensics network to outperform a CNNConvolutional Neural Networks (CNN) with a similar design and to be from 5 to 11 times smaller than a CNNConvolutional Neural Networks (CNN) with similar performance.","tags":[],"title":"Capsule-Forensics Networks for Deepfake Detection","type":"publication"},{"authors":["Haoyu Li","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948770,"objectID":"beae4779c04117d592cc57210bf33346","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/li-22-e-interspeech/","publishdate":"2022-11-20T12:52:50.606786Z","relpermalink":"/nii-yamagishilab/publication/li-22-e-interspeech/","section":"publication","summary":"","tags":[],"title":"DDS: A new device-degraded speech dataset for speech enhancement","type":"publication"},{"authors":["Xin Wang","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948773,"objectID":"177ea1612e654c6e17d1bb83112b7f85","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9746204/","publishdate":"2022-11-20T12:52:53.505524Z","relpermalink":"/nii-yamagishilab/publication/9746204/","section":"publication","summary":"","tags":[],"title":"Estimating the Confidence of Speech Spoofing Countermeasure","type":"publication"},{"authors":["Ruben Tolosana","Christian Rathgeb","Ruben Vera-Rodriguez","Christoph Busch","Luisa Verdoliva","Siwei Lyu","Huy Hong Nguyen","Junichi Yamagishi","Isao Echizen","Peter Rot","Klemen Grm","Vitomir Štruc","Antitza Dantcheva","Zahid Akhtar","Sergio Romero-Tapiador","Julian Fierrez","Aythami Morales","Javier Ortega-Garcia","Els Kindt","Catherine Jasserand","Tarmo Kalvet","Marek Tiits"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672500189,"objectID":"9accc0671198e192fb521cc6d51d506a","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/tolosana-2022/","publishdate":"2022-12-31T15:23:08.92279Z","relpermalink":"/nii-yamagishilab/publication/tolosana-2022/","section":"publication","summary":"Recently, digital face manipulationDigital face manipulation and its detection have sparked large interest in industry and academia around the world. Numerous approaches have been proposed in the literature to create realistic face manipulationsFace manipulation, such as DeepFakesDeepFake and face morphs. To the human eye manipulated images and videos can be almost indistinguishable from real content. Although impressive progress has been reported in the automatic detection of such face manipulationsFace manipulation, this research field is often considered to be a cat and mouse game. This chapter briefly discusses the state of the art of digital face manipulationDigital face manipulation and detection. Issues and challenges that need to be tackled by the research community are summarized, along with future trends in the field.","tags":[],"title":"Future Trends in Digital Face Manipulation and Detection","type":"publication"},{"authors":["Erica Cooper","Wen-Chin Huang","Tomoki Toda","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948774,"objectID":"1ebdc0f03f8f74d3a65114df1974b974","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9746395/","publishdate":"2022-11-20T12:52:53.861683Z","relpermalink":"/nii-yamagishilab/publication/9746395/","section":"publication","summary":"","tags":[],"title":"Generalization Ability of MOS Prediction Networks","type":"publication"},{"authors":["Xin Wang","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948772,"objectID":"50b46476f0be9e0cfb6a1ebdf0662541","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/wang-22-odyssey/","publishdate":"2022-11-20T12:52:52.053779Z","relpermalink":"/nii-yamagishilab/publication/wang-22-odyssey/","section":"publication","summary":"","tags":[],"title":"Investigating Self-Supervised Front Ends for Speech Spoofing Countermeasures","type":"publication"},{"authors":["Chang Zeng","XiaoXiao Miao","Xin Wang","Erica Cooper","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948767,"objectID":"64c5fff771c898b49397b191ec07d06f","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/zeng-2022-joint/","publishdate":"2022-11-20T12:52:47.732097Z","relpermalink":"/nii-yamagishilab/publication/zeng-2022-joint/","section":"publication","summary":"","tags":[],"title":"Joint Speaker Encoder and Neural Back-end Model for Fully End-to-End Automatic Speaker Verification with Multiple Enrollment Utterances","type":"publication"},{"authors":["XiaoXiao Miao","Xin Wang","Erica Cooper","Junichi Yamagishi","Natalia Tomashenko"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948771,"objectID":"8c763f2edbbaa7e5b7cb4904a8a33306","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/miao-22-odyssey/","publishdate":"2022-11-20T12:52:51.333192Z","relpermalink":"/nii-yamagishilab/publication/miao-22-odyssey/","section":"publication","summary":"","tags":[],"title":"Language-Independent Speaker Anonymization Approach Using Self-Supervised Pre-Trained Models","type":"publication"},{"authors":["Huy Hong Nguyen","Sebastien Marcel","Junichi Yamagishi","Isao Echizen"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948772,"objectID":"46f16ee7e66333c8beff3675329865bb","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9758063/","publishdate":"2022-11-20T12:52:52.419931Z","relpermalink":"/nii-yamagishilab/publication/9758063/","section":"publication","summary":"","tags":[],"title":"Master Face Attacks on Face Recognition Systems","type":"publication"},{"authors":["Canasai Kruengkrai","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948769,"objectID":"15e2246ea850b21ac2205876615a6be9","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/kruengkrai-2022-mitigating/","publishdate":"2022-11-20T12:52:49.537811Z","relpermalink":"/nii-yamagishilab/publication/kruengkrai-2022-mitigating/","section":"publication","summary":"","tags":[],"title":"Mitigating the Diminishing Effect of Elastic Weight Consolidation","type":"publication"},{"authors":["Cheng-I Jeff Lai","Erica Cooper","Yang Zhang","Shiyu Chang","Kaizhi Qian","Yi-Lun Liao","Yung-Sung Chuang","Alexander H. Liu","Junichi Yamagishi","David Cox","James Glass"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948774,"objectID":"7295f017114e5d625f442c55ee15b7b9","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9747728/","publishdate":"2022-11-20T12:52:54.224937Z","relpermalink":"/nii-yamagishilab/publication/9747728/","section":"publication","summary":"","tags":[],"title":"On the Interplay between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis","type":"publication"},{"authors":["Anssi Kanervisto","Ville Hautamäki","Tomi Kinnunen","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948775,"objectID":"bf014dd606165f8766a5fdd17d5be300","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9664367/","publishdate":"2022-11-20T12:52:55.316613Z","relpermalink":"/nii-yamagishilab/publication/9664367/","section":"publication","summary":"","tags":[],"title":"Optimizing Tandem Speaker Verification and Anti-Spoofing Systems","type":"publication"},{"authors":["Trung-Nghia Le","Huy Hong Nguyen","Junichi Yamagishi","Isao Echizen"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672500190,"objectID":"e15bf1fc9f48ad414d1090589095f09b","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/le-2022/","publishdate":"2022-12-31T15:23:10.1356Z","relpermalink":"/nii-yamagishilab/publication/le-2022/","section":"publication","summary":"Recent advances in deep learning have led to substantial improvements in deepfake generation, resulting in fake media with a more realistic appearance. Although deepfake media have potential application in a wide range of areas and are drawing much attention from both the academic and industrial communities, it also leads to serious social and criminal concerns. This chapter explores the evolution of and challenges in deepfake generation and detection. It also discusses possible ways to improve the robustness of deepfake detection for a wide variety of media (e.g., in-the-wild images and videos). Finally, it suggests a focus for future fake media research.","tags":[],"title":"Robust Deepfake on Unrestricted Media: Generation and Detection","type":"publication"},{"authors":["Chang Zeng","Lin Zhang","Meng Liu","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948771,"objectID":"c2abb11e75f31b801a12e45f71af679c","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/zeng-22-interspeech/","publishdate":"2022-11-20T12:52:50.974254Z","relpermalink":"/nii-yamagishilab/publication/zeng-22-interspeech/","section":"publication","summary":"","tags":[],"title":"Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022","type":"publication"},{"authors":["Cheng-Hung Hu","Yu-Huai Peng","Junichi Yamagishi","Yu Tsao","Hsin-Min Wang"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948773,"objectID":"c4975ab4c747daf1794a45355e2e3896","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9716822/","publishdate":"2022-11-20T12:52:53.140244Z","relpermalink":"/nii-yamagishilab/publication/9716822/","section":"publication","summary":"","tags":[],"title":"SVSNet: An End-to-End Speaker Voice Similarity Assessment Model","type":"publication"},{"authors":["Wen Chin Huang","Erica Cooper","Yu Tsao","Hsin-Min Wang","Tomoki Toda","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948770,"objectID":"1d23b0baaea39fe2d4caae480e4759a7","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/huang-22-f-interspeech/","publishdate":"2022-11-20T12:52:50.247958Z","relpermalink":"/nii-yamagishilab/publication/huang-22-f-interspeech/","section":"publication","summary":"","tags":[],"title":"The VoiceMOS Challenge 2022","type":"publication"},{"authors":["Natalia Tomashenko","Xin Wang","Emmanuel Vincent","Jose Patino","Brij Mohan Lal Srivastava","Paul-Gauthier Noé","Andreas Nautsch","Nicholas Evans","Junichi Yamagishi","Benjamin O’Brien"," others"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948772,"objectID":"ae0ffaa99ed85114c9150e8a4eeb8f73","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/tomashenko-2022-voiceprivacy/","publishdate":"2022-11-20T12:52:52.781113Z","relpermalink":"/nii-yamagishilab/publication/tomashenko-2022-voiceprivacy/","section":"publication","summary":"","tags":[],"title":"The VoicePrivacy 2020 Challenge: Results and findings","type":"publication"},{"authors":["Xuan Shi","Erica Cooper","Junichi Yamagishi"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948775,"objectID":"3a1ed5bb960fad110b669c462f144b2d","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9670718/","publishdate":"2022-11-20T12:52:54.954009Z","relpermalink":"/nii-yamagishilab/publication/9670718/","section":"publication","summary":"","tags":[],"title":"Use of Speaker Recognition Approaches for Learning and Evaluating Embedding Representations of Musical Instrument Sounds","type":"publication"},{"authors":null,"categories":null,"content":"VoiceMOS The VoiceMOS Challenge 2022 has ended! Material from the challenge will remain available online. Read our summary paper of the challenge here.\nIntroduction Human listening tests are the gold standard for evaluating synthesized speech. Objective measures of speech quality have low correlation with human ratings, and the generalization abilities of current data-driven quality prediction systems suffer significantly from domain mismatch. The VoiceMOS Challenge aims to encourage research in the area of automatic prediction of Mean Opinion Scores (MOS) for synthesized speech.\nOrganisers Wen-Chin Huang (Nagoya University, Japan) Erica Cooper (National Institute of Informatics, Japan) Yu Tsao (Academia Sinica, Taiwan) Hsin-Min Wang (Academia Sinica, Taiwan) Tomoki Toda (Nagoya University, Japan) Junichi Yamagishi (National Institute of Informatics, Japan) ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"41c26400c419832677735d29e3f803c0","permalink":"https://zlin0.github.io/nii-yamagishilab/challenges/voicemos/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/challenges/voicemos/","section":"challenges","summary":"The VoiceMOS Challenge is a shared task using common datasets for MOS prediction.","tags":["VoiceMos"],"title":"VoiceMOS","type":"challenges"},{"authors":null,"categories":null,"content":"VoicePrivacy Introduction Formed in 2020, the VoicePrivacy initiative is spearheading the effort to develop privacy preservations solutions for speech technology. We aim to foster progress in the development of anonymisation and pseudonymisation solutions which suppress personally identifiable information contained within recordings of speech while preserving linguistic content and speech quality/naturalness. VoicePrivacy takes the form of a competitive benchmarking challenge, with common datasets, protocols and metrics. The first edition of VoicePrivacy was held in 2020, culminating in a special sessions held at INTERSPEECH 2020 and Odyssey 2020, and a special issue published in Elsevier Computer Speech and Language. VoicePrivacy 2022 is the second edition starts from March 2022.\nOrganisers (in alphabetical order)\nJean-François Bonastre - University of Avignon - LIA, France Pierre Champion - Inria, France Nicholas Evans - EURECOM, France Xiaoxiao Miao - NII, Japan Hubert Nourtel - Inria, France Massimiliano Todisco - EURECOM, France Natalia Tomashenko - University of Avignon - LIA, France Emmanuel Vincent - Inria, France Xin Wang - NII, Japan Junichi Yamagishi - NII, Japan and University of Edinburgh, UK ","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"dd001f5fc221a559d67103e9dcef502b","permalink":"https://zlin0.github.io/nii-yamagishilab/challenges/voiceprivacy/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/challenges/voiceprivacy/","section":"challenges","summary":"The VoicePrivacy initiative is spearheading the effort to develop privacy preservations solutions for speech technology.","tags":["VoicePrivacy"],"title":"VoicePrivacy","type":"challenges"},{"authors":["Canasai Kruengkrai","Junichi Yamagishi","Xin Wang"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676819037,"objectID":"259053bf280653f26938381d3094d55d","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/kruengkrai-etal-2021-multi/","publishdate":"2023-02-19T15:19:00.54436Z","relpermalink":"/nii-yamagishilab/publication/kruengkrai-etal-2021-multi/","section":"publication","summary":"","tags":[],"title":"A Multi-Level Attention Model for Evidence-Based Fact Checking","type":"publication"},{"authors":["Xin Wang","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948780,"objectID":"abedc1cffc85ec35634762e884c13fb0","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/wang-21-fa-interspeech/","publishdate":"2022-11-20T12:53:00.063444Z","relpermalink":"/nii-yamagishilab/publication/wang-21-fa-interspeech/","section":"publication","summary":"","tags":[],"title":"A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection","type":"publication"},{"authors":["Lin Zhang","Xin Wang","Erica Cooper","Junichi Yamagishi","Jose Patino","Nicholas Evans"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948779,"objectID":"175023e5f44464b986229456822d9068","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/zhang-21-ca-interspeech/","publishdate":"2022-11-20T12:52:59.705422Z","relpermalink":"/nii-yamagishilab/publication/zhang-21-ca-interspeech/","section":"publication","summary":"","tags":[],"title":"An Initial Investigation for Detecting Partially Spoofed Audio","type":"publication"},{"authors":["Andreas Nautsch","Xin Wang","Nicholas Evans","Tomi H. Kinnunen","Ville Vestman","Massimiliano Todisco","Héctor Delgado","Md Sahidullah","Junichi Yamagishi","Kong Aik Lee"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948780,"objectID":"7f92130a8a5d03443818de88584d88a6","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9358099/","publishdate":"2022-11-20T12:53:00.422305Z","relpermalink":"/nii-yamagishilab/publication/9358099/","section":"publication","summary":"","tags":[],"title":"ASVspoof 2019: Spoofing Countermeasures for the Detection of Synthesized, Converted and Replayed Speech","type":"publication"},{"authors":["Junichi Yamagishi","Xin Wang","Massimiliano Todisco","Md Sahidullah","Jose Patino","Andreas Nautsch","Xuechen Liu","Kong Aik Lee","Tomi Kinnunen","Nicholas Evans","Héctor Delgado"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948776,"objectID":"fe6ec06fd947498be001c16a2d0eed95","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/yamagishi-21-asvspoof/","publishdate":"2022-11-20T12:52:56.74999Z","relpermalink":"/nii-yamagishilab/publication/yamagishi-21-asvspoof/","section":"publication","summary":"","tags":[],"title":"ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection","type":"publication"},{"authors":["Jean-François Bonastre","Héctor Delgado","Nicholas Evans","Tomi Kinnunen","Kong Aik Lee","Xuechen Liu","Andreas Nautsch","‪Paul-Gauthier Noé‬","Jose Patino","Md Sahidullah","Brij Mohan Lal Srivastava","Massimiliano Todisco","Natalia Tomashenko","Emmanuel Vincent","Xin Wang","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948776,"objectID":"8dae7009871cbc08b32c3995f0be31ac","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/bonastre-21-spsc/","publishdate":"2022-11-20T12:52:56.394681Z","relpermalink":"/nii-yamagishilab/publication/bonastre-21-spsc/","section":"publication","summary":"","tags":[],"title":"Benchmarking and challenges in security and privacy for voice biometrics","type":"publication"},{"authors":["Yang Ai","Haoyu Li","Xin Wang","Junichi Yamagishi","Zhenhua Ling"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948782,"objectID":"aae19708afae62042317c2a9a17ffe57","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9383611/","publishdate":"2022-11-20T12:53:02.18928Z","relpermalink":"/nii-yamagishilab/publication/9383611/","section":"publication","summary":"","tags":[],"title":"Denoising-and-Dereverberation Hierarchical Neural Vocoder for Robust Waveform Generation","type":"publication"},{"authors":["Khanh-Duv Nguyen","Huv H. Nguyen","Trung-Nghia Le","Junichi Yamagishi","Isao Echizen"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948775,"objectID":"293b9f336c0dc6930aa7f312b120c13e","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9667046/","publishdate":"2022-11-20T12:52:55.677145Z","relpermalink":"/nii-yamagishilab/publication/9667046/","section":"publication","summary":"","tags":[],"title":"Effectiveness of Detection-based and Regression-based Approaches for Estimating Mask-Wearing Ratio","type":"publication"},{"authors":["Yusuke Yasuda","Xin Wang","Junichi Yamagishd"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948780,"objectID":"38a37dc519e6869ab582178d85101be3","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9414499/","publishdate":"2022-11-20T12:53:00.774243Z","relpermalink":"/nii-yamagishilab/publication/9414499/","section":"publication","summary":"","tags":[],"title":"End-to-End Text-to-Speech Using Latent Duration Based on VQ-VAE","type":"publication"},{"authors":["Haoyu Li","Yang Ai","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948781,"objectID":"dda2c7f52bdeb3eeb992686c4e421baf","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9383507/","publishdate":"2022-11-20T12:53:01.828429Z","relpermalink":"/nii-yamagishilab/publication/9383507/","section":"publication","summary":"","tags":[],"title":"Enhancing Low-Quality Voice Recordings Using Disentangled Channel Factor and Neural Waveform Model","type":"publication"},{"authors":["Jennifer Williams","Jason Fong","Erica Cooper","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948779,"objectID":"ad4e82ca1695e8d20b19a8cee7afb34d","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/williams-21-ssw/","publishdate":"2022-11-20T12:52:58.993419Z","relpermalink":"/nii-yamagishilab/publication/williams-21-ssw/","section":"publication","summary":"","tags":[],"title":"Exploring Disentanglement with Multilingual and Monolingual VQ-VAE","type":"publication"},{"authors":["Erica Cooper","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948778,"objectID":"4bf59de148f6e8f1de7b786f5c35a71d","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/cooper-21-ssw/","publishdate":"2022-11-20T12:52:58.239502Z","relpermalink":"/nii-yamagishilab/publication/cooper-21-ssw/","section":"publication","summary":"","tags":[],"title":"How do Voices from Past Speech Synthesis Challenges Compare Today?","type":"publication"},{"authors":["Shuhei Kato","Yusuke Yasuda","Xin Wang","Erica Cooper","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948781,"objectID":"07cb39b0b425f9ae534894532701a5c1","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9414175/","publishdate":"2022-11-20T12:53:01.479497Z","relpermalink":"/nii-yamagishilab/publication/9414175/","section":"publication","summary":"","tags":[],"title":"How Similar or Different is Rakugo Speech Synthesizer to Professional Performers?","type":"publication"},{"authors":["Jennifer Williams","Yi Zhao","Erica Cooper","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948781,"objectID":"132c6fa04ad2902f260559cffdf5aa37","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9413543/","publishdate":"2022-11-20T12:53:01.127144Z","relpermalink":"/nii-yamagishilab/publication/9413543/","section":"publication","summary":"","tags":[],"title":"Learning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm","type":"publication"},{"authors":["Haoyu Li","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948779,"objectID":"9d8e2cc9aa609322cb53c8ff30e76712","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9536406/","publishdate":"2022-11-20T12:52:59.344943Z","relpermalink":"/nii-yamagishilab/publication/9536406/","section":"publication","summary":"","tags":[],"title":"Multi-Metric Optimization Using Generative Adversarial Networks for Near-End Speech Intelligibility Enhancement","type":"publication"},{"authors":["Lin Zhang","Xin Wang","Erica Cooper","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948777,"objectID":"2b7b0e69c16c3cac45f66a75ebaf3a87","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/zhang-21-asvspoof/","publishdate":"2022-11-20T12:52:57.133123Z","relpermalink":"/nii-yamagishilab/publication/zhang-21-asvspoof/","section":"publication","summary":"","tags":[],"title":"Multi-task Learning in Utterance-level and Segmental-level Spoof Detection","type":"publication"},{"authors":["Trung-Nghia Le","Huy Hong Nguyen","Junichi Yamagishi","Isao Echizen"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948777,"objectID":"47c4866217f2b53eadb8bedc872e69e3","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/9711250/","publishdate":"2022-11-20T12:52:57.484095Z","relpermalink":"/nii-yamagishilab/publication/9711250/","section":"publication","summary":"","tags":[],"title":"OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild","type":"publication"},{"authors":["Jennifer Williams","Junichi Yamagishi"," Paul-Gauthier","Cassia Valentini-Botinhao","Jean-François Bonastre"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948776,"objectID":"9f279964c7f0ca1d3ebed8744367dbbb","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/williams-21-spsc/","publishdate":"2022-11-20T12:52:56.035461Z","relpermalink":"/nii-yamagishilab/publication/williams-21-spsc/","section":"publication","summary":"","tags":[],"title":"Revisiting Speech Content Privacy","type":"publication"},{"authors":["Erica Cooper","Xin Wang","Junichi Yamagishi"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668948778,"objectID":"cd994f411d0b54551c083cd41004dcad","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/cooper-21-b-ssw/","publishdate":"2022-11-20T12:52:58.633825Z","relpermalink":"/nii-yamagishilab/publication/cooper-21-b-ssw/","section":"publication","summary":"","tags":[],"title":"Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis","type":"publication"},{"authors":["Md Sahidullah","Héctor Delgado","Massimiliano Todisco","Tomi Kinnunen","Nicholas Evans","Junichi Yamagishi","Kong-Aik Lee"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672500189,"objectID":"2eae2287b6bf59c46fc5b9e1c6789fc1","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/sahidullah-2019/","publishdate":"2022-12-31T15:23:09.332656Z","relpermalink":"/nii-yamagishilab/publication/sahidullah-2019/","section":"publication","summary":"Over the past few years, significant progress has been made in the field of presentation attack detection (PAD) for automatic speaker recognition (ASV). This includes the development of new speech corpora, standard evaluation protocols and advancements in front-end feature extraction and back-end classifiers. The use of standard databases and evaluation protocols has enabled for the first time the meaningful benchmarking of different PAD solutions. This chapter summarises the progress, with a focus on studies completed in the last 3 years. The article presents a summary of findings and lessons learned from two ASVspoof challenges, the first community-led benchmarking efforts. These show that ASV PAD remains an unsolved problem and that further attention is required to develop generalised PAD solutions which have potential to detect diverse and previously unseen spoofing attacks.","tags":[],"title":"Introduction to Voice Presentation Attack Detection and Recent Advances","type":"publication"},{"authors":["Keiichi Tokuda","Akinobu Lee","Yoshihiko Nankaku","Keiichiro Oura","Kei Hashimoto","Daisuke Yamamoto","Ichi Takumi","Takahiro Uchiya","Shuhei Tsutsumi","Steve Renals","Junichi Yamagishi"],"categories":[],"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672500189,"objectID":"584cd824e85c8cb7063457c6311cc665","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/tokuda-2017/","publishdate":"2022-12-31T15:23:09.739813Z","relpermalink":"/nii-yamagishilab/publication/tokuda-2017/","section":"publication","summary":"This chapter introduces the idea of user-generated dialogueUDialogue technologycontent and describes our experimental exploration aimed at clarifying the mechanism and conditions that makes it workable in practice. One of the attractive points of a speech interface is to provide a vivid sense of interactivity that cannot be achieved with a text interface alone. This study proposes a framework that spoken dialogue systems are separated into content that can be produced and modified by users, and the systems that drive the content, and seek to clarify (1) the requirements of systems that enable the creation of attractive spoken dialogue, and (2) the conditions for the active generation of attractive dialogue content by users, while attempting to establish a method for realizing them. Experiments for validating user dialogue content generation were performed by installing interactive digital signage with a speech interface in public spaces as a dialogue device, and implementing a content generation environment for users via the Internet. The proposed framework is expected to lead to a breakthrough in the spread of using speech technology.","tags":[],"title":"User Generated Dialogue Systems: uDialogue","type":"publication"},{"authors":["順一 山岸","恵一 徳田","智基 戸田","よしこ みわ"," 国立情報学研究所"],"categories":[],"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672500191,"objectID":"46349549d602faf21de004b4a4c23473","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/bb-18335564/","publishdate":"2022-12-31T15:23:10.931204Z","relpermalink":"/nii-yamagishilab/publication/bb-18335564/","section":"publication","summary":"","tags":[],"title":"おしゃべりなコンピュータ : 音声合成技術の現在と未来","type":"publication"},{"authors":["Nicholas Evans","Tomi Kinnunen","Junichi Yamagishi","Zhizheng Wu","Federico Alegre","Phillip De Leon"],"categories":[],"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672500188,"objectID":"348bf12ed2a4343389d66f73db4d0ca6","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/evans-2014/","publishdate":"2022-12-31T15:23:08.104441Z","relpermalink":"/nii-yamagishilab/publication/evans-2014/","section":"publication","summary":"Progress in the development of spoofing countermeasures for automatic speaker recognition  is less advanced than equivalent work related to other biometric modalities. This chapter outlines the potential for even state-of-the-art automatic speaker recognition systems to be spoofed. While the use of a multitude of different datasets, protocols and metrics complicates the meaningful comparison of different vulnerabilities, we review previous work related to impersonation, replay, speech synthesis and voice conversion spoofing attacks. The article also presents an analysis of the early work to develop spoofing countermeasures. The literature shows that there is significant potential for automatic speaker verification systems to be spoofed, that significant further work is required to develop generalised countermeasures, that there is a need for standard datasets, evaluation protocols and metrics and that greater emphasis should be placed on text-dependent scenarios.","tags":[],"title":"Speaker Recognition Anti-spoofing","type":"publication"},{"authors":["Sarah Creer","Phil Green","Stuart Cunningham","Junichi Yamagishi"],"categories":[],"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672500191,"objectID":"956efee20036a8404fd222573334f29c","permalink":"https://zlin0.github.io/nii-yamagishilab/publication/creer-2010-building/","publishdate":"2022-12-31T15:23:11.325808Z","relpermalink":"/nii-yamagishilab/publication/creer-2010-building/","section":"publication","summary":"","tags":[],"title":"Building personalized synthetic voices for individuals with dysarthria using the HTS toolkit","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://zlin0.github.io/nii-yamagishilab/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"62fea3e05460f95efe6cc5b6342d9794","permalink":"https://zlin0.github.io/nii-yamagishilab/databases/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/databases/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"32a76a54894d6ac47709b46a0108f658","permalink":"https://zlin0.github.io/nii-yamagishilab/grants/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/grants/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://zlin0.github.io/nii-yamagishilab/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/people/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8d41a9ddd0e7e49753dab2e66e906918","permalink":"https://zlin0.github.io/nii-yamagishilab/recruitments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/recruitments/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b0d61e5cbb7472bf320bf0ef2aaeb977","permalink":"https://zlin0.github.io/nii-yamagishilab/tour/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/nii-yamagishilab/tour/","section":"","summary":"","tags":null,"title":"Tour","type":"widget_page"}]