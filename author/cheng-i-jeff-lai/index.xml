<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cheng-I Jeff Lai | NII Yamagishi's Lab</title><link>https://zlin0.github.io/nii-yamagishilab/author/cheng-i-jeff-lai/</link><atom:link href="https://zlin0.github.io/nii-yamagishilab/author/cheng-i-jeff-lai/index.xml" rel="self" type="application/rss+xml"/><description>Cheng-I Jeff Lai</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 19 Dec 2022 00:00:00 +0000</lastBuildDate><image><url>https://zlin0.github.io/nii-yamagishilab/media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_512x512_fill_lanczos_center_3.png</url><title>Cheng-I Jeff Lai</title><link>https://zlin0.github.io/nii-yamagishilab/author/cheng-i-jeff-lai/</link></image><item><title>[Dec. 20] Textless Phrase-Structure Induction from Visually-Grounded Speech</title><link>https://zlin0.github.io/nii-yamagishilab/talks/2022-12-20-jeff/</link><pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talks/2022-12-20-jeff/</guid><description>&lt;p>We study phrase structure induction from visually-grounded speech without intermediate text or text pre-trained models. The core idea is to first segment the speech waveform into sequences of word segments, then induce phrase structure based on the inferred segment-level continuous representations. To this end, we present the Audio-Visual Neural Syntax Learner (AV-NSL) that learns non-trivial phrase structure by listening to audio and looking at images, without ever reading text. Experiments on SpokenCOCO, the spoken version of MSCOCO with paired images and spoken captions, show that AV-NSL infers meaningful phrase structures similar to those learned from naturally-supervised text parsing, quantitatively and qualitatively. The findings in this paper extend prior work in unsupervised language acquisition from speech and grounded grammar induction, and manifest one possibility of bridging the gap between the two fields.&lt;/p></description></item><item><title>On the Interplay between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9747728/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9747728/</guid><description/></item></channel></rss>