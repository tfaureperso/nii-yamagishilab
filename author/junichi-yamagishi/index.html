<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=../../css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=../../css/wowchemy.f3c28803eb07089619aac681dc14ff29.css><link rel=stylesheet href=../../css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=../../css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Junichi Yamagishi"><meta name=description content="Professor"><link rel=alternate hreflang=en-us href=https://zlin0.github.io/nii-yamagishilab/author/junichi-yamagishi/><link rel=canonical href=https://zlin0.github.io/nii-yamagishilab/author/junichi-yamagishi/><link rel=manifest href=../../manifest.webmanifest><link rel=icon type=image/png href=../../media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=../../media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#3f51b5"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@yamagishilab"><meta property="twitter:creator" content="@yamagishilab"><meta property="twitter:image" content="https://zlin0.github.io/nii-yamagishilab/author/junichi-yamagishi/avatar_huce29434914b8236f788ecea83bbd5cb5_211605_270x270_fill_q75_lanczos_center.jpg"><meta property="og:site_name" content="NII Yamagishi's Lab"><meta property="og:url" content="https://zlin0.github.io/nii-yamagishilab/author/junichi-yamagishi/"><meta property="og:title" content="Junichi Yamagishi | NII Yamagishi's Lab"><meta property="og:description" content="Professor"><meta property="og:image" content="https://zlin0.github.io/nii-yamagishilab/author/junichi-yamagishi/avatar_huce29434914b8236f788ecea83bbd5cb5_211605_270x270_fill_q75_lanczos_center.jpg"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-01-01T00:00:00+00:00"><link rel=alternate href=../../author/junichi-yamagishi/index.xml type=application/rss+xml title="NII Yamagishi's Lab"><title>Junichi Yamagishi | NII Yamagishi's Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=fdbbe40a945160dd41e945575d9ac121><script src=../../js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=../../>NII Yamagishi's Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=../../>NII Yamagishi's Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=../../news><span>News</span></a></li><li class=nav-item><a class=nav-link href=../../people><span>Members</span></a></li><li class=nav-item><a class=nav-link href=../../publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=../../challenges><span>Challenges</span></a></li><li class=nav-item><a class=nav-link href=../../databases><span>Databases</span></a></li><li class=nav-item><a class=nav-link href=../../talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=../../grants><span>Grants</span></a></li><li class=nav-item><a class=nav-link href=../../recruitments><span>Recruitments</span></a></li><li class=nav-item><a class=nav-link href=../../contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><section id=profile-page class=pt-5><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=../../author/junichi-yamagishi/avatar_huce29434914b8236f788ecea83bbd5cb5_211605_270x270_fill_q75_lanczos_center.jpg alt="Junichi Yamagishi"><div class=portrait-title><h2>Junichi Yamagishi</h2><h3>Professor</h3><h3><span>National Institute of Informatics</span></h3></div><ul class=network-icon aria-hidden=true><li><a href="https://scholar.google.com/citations?user=nRrdjtwAAAAJ&hl=en&oi=ao" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar big-icon"></i></a></li><li><a href=https://researchmap.jp/read0205283 target=_blank rel=noopener aria-label=external-link-alt><i class="fas fa-external-link-alt big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><div class=article-style><p>Junichi Yamagishi received the Ph.D. degree from Tokyo Institute of Technology in 2006 for a thesis that pioneered speaker-adaptive speech synthesis. He is currently a Professor with the National Institute of Informatics, Tokyo, Japan, and also a Senior Research Fellow with the Centre for Speech Technology Research, University of Edinburgh, Edinburgh, U.K. Since 2006, he has authored and co-authored more than 250 refereed papers in international journals and conferences.</p><p>He was an area coordinator at Interspeech 2012. He was one of organizers for special sessions on “Spoofing and Countermeasures for Automatic Speaker Verification” at Interspeech 2013, “ASVspoof evaluation” at Interspeech 2015, “Voice conversion challenge 2016” at Interspeech 2016, “2nd ASVspoof evaluation” at Interspeech 2017, and “Voice conversion challenge 2018” at Speaker Odyssey 2018. He is currently an organizing committee for ASVspoof 2019, an organizing committee for ISCA the 10th ISCA Speech Synthesis Workshop 2019, a technical program committee for IEEE ASRU 2019, and an award committee for ISCA Speaker Odyssey 2020.</p><p>He was a member of IEEE Speech and Language Technical Committee. He was also an Associate Editor of the IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING and a Lead Guest Editor for the IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING special issue on Spoofing and Countermeasures for Automatic Speaker Verification. He is currently a guest editor for Computer Speech and Language special issue on speaker and language characterization and recognition: voice modeling, conversion, synthesis and ethical aspects. He also serves as a chairperson of ISCA SynSIG currently.</p><p>He was the recipient of the Tejima Prize as the best Ph.D. thesis of Tokyo Institute of Technology in 2007. He received the Itakura Prize from the Acoustic Society of Japan in 2010, the Kiyasu Special Industrial Achievement Award from the Information Processing Society of Japan in 2013, the Young Scientists’ Prize from the Minister of Education, Science and Technology in 2014, the JSPS Prize from Japan Society for the Promotion of Science in 2016, and Docomo mobile science award from Mobile communication fund in 2018.</p></div><div class=row><div class=col-md-7><div class=section-subheading>Education</div><ul class="ul-edu fa-ul mb-0"><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>PhD in Average-voice speech synthesis, 2006</p><p class=institution>Tokyo Institute of Technology, Japan</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>MSc in Informatics, 2003</p><p class=institution>Tokyo Institute of Technology Japan</p></div></li><li><i class="fa-li fas fa-graduation-cap"></i><div class=description><p class=course>BSc in Computer science, 2002</p><p class=institution>Tokyo Institute of Technology, Japan</p></div></li></ul></div></div></div></div><div class="article-widget content-widget-hr"><h3>Latest</h3><ul><li><a href=../../publication/zhang-2022-partial-spoof/>The PartialSpoof Database and Countermeasures for the Detection of Short Fake Speech Segments Embedded in an Utterance</a></li><li><a href=../../publication/wang-2022/>A Practical Guide to Logical Access Voice Presentation Attack Detection</a></li><li><a href=../../publication/miao-22-interspeech/>Analyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions</a></li><li><a href=../../publication/9746688/>Attention Back-End for Automatic Speaker Verification with Multiple Enrollment Utterances</a></li><li><a href=../../publication/tak-22-odyssey/>Automatic Speaker Verification Spoofing and Deepfake Detection Using Wav2vec 2.0 and Data Augmentation</a></li><li><a href=../../publication/shi-2022-can/>Can Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems?</a></li><li><a href=../../publication/nguyen-2022/>Capsule-Forensics Networks for Deepfake Detection</a></li><li><a href=../../publication/li-22-e-interspeech/>DDS: A new device-degraded speech dataset for speech enhancement</a></li><li><a href=../../publication/9746204/>Estimating the Confidence of Speech Spoofing Countermeasure</a></li><li><a href=../../publication/tolosana-2022/>Future Trends in Digital Face Manipulation and Detection</a></li><li><a href=../../publication/9746395/>Generalization Ability of MOS Prediction Networks</a></li><li><a href=../../publication/noe-2022-hiding/>Hiding speaker's sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline</a></li><li><a href=../../publication/wang-22-odyssey/>Investigating Self-Supervised Front Ends for Speech Spoofing Countermeasures</a></li><li><a href=../../publication/li-2022-joint/>Joint Noise Reduction and Listening Enhancement for Full-End Speech Enhancement</a></li><li><a href=../../publication/zeng-2022-joint/>Joint Speaker Encoder and Neural Back-end Model for Fully End-to-End Automatic Speaker Verification with Multiple Enrollment Utterances</a></li><li><a href=../../publication/miao-22-odyssey/>Language-Independent Speaker Anonymization Approach Using Self-Supervised Pre-Trained Models</a></li><li><a href=../../publication/9758063/>Master Face Attacks on Face Recognition Systems</a></li><li><a href=../../publication/kruengkrai-2022-mitigating/>Mitigating the Diminishing Effect of Elastic Weight Consolidation</a></li><li><a href=../../publication/9747728/>On the Interplay between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis</a></li><li><a href=../../publication/9664367/>Optimizing Tandem Speaker Verification and Anti-Spoofing Systems</a></li><li><a href=../../publication/chen-2022-outlier/>Outlier-Aware Training for Improving Group Accuracy Disparities</a></li><li><a href=../../publication/le-2022/>Robust Deepfake on Unrestricted Media: Generation and Detection</a></li><li><a href=../../publication/wang-2022-spoofed/>Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders</a></li><li><a href=../../publication/zeng-22-interspeech/>Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022</a></li><li><a href=../../publication/9716822/>SVSNet: An End-to-End Speaker Voice Similarity Assessment Model</a></li><li><a href=../../publication/zhang-2022-partialspoof/>The PartialSpoof Database and Countermeasures for the Detection of Short Generated Audio Segments Embedded in a Speech Utterance</a></li><li><a href=../../publication/huang-22-f-interspeech/>The VoiceMOS Challenge 2022</a></li><li><a href=../../publication/tomashenko-2022-voiceprivacy/>The VoicePrivacy 2020 Challenge: Results and findings</a></li><li><a href=../../publication/9670718/>Use of Speaker Recognition Approaches for Learning and Evaluating Embedding Representations of Musical Instrument Sounds</a></li><li><a href=../../publication/wang-21-fa-interspeech/>A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection</a></li><li><a href=../../publication/kruengkrai-2021-multi/>A multi-level attention model for evidence-based fact checking</a></li><li><a href=../../publication/zhang-21-ca-interspeech/>An Initial Investigation for Detecting Partially Spoofed Audio</a></li><li><a href=../../publication/9358099/>ASVspoof 2019: Spoofing Countermeasures for the Detection of Synthesized, Converted and Replayed Speech</a></li><li><a href=../../publication/yamagishi-21-asvspoof/>ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection</a></li><li><a href=../../publication/bonastre-21-spsc/>Benchmarking and challenges in security and privacy for voice biometrics</a></li><li><a href=../../publication/9383611/>Denoising-and-Dereverberation Hierarchical Neural Vocoder for Robust Waveform Generation</a></li><li><a href=../../publication/9667046/>Effectiveness of Detection-based and Regression-based Approaches for Estimating Mask-Wearing Ratio</a></li><li><a href=../../publication/9383507/>Enhancing Low-Quality Voice Recordings Using Disentangled Channel Factor and Neural Waveform Model</a></li><li><a href=../../publication/williams-21-ssw/>Exploring Disentanglement with Multilingual and Monolingual VQ-VAE</a></li><li><a href=../../publication/cooper-21-ssw/>How do Voices from Past Speech Synthesis Challenges Compare Today?</a></li><li><a href=../../publication/9414175/>How Similar or Different is Rakugo Speech Synthesizer to Professional Performers?</a></li><li><a href=../../publication/9413543/>Learning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm</a></li><li><a href=../../publication/9536406/>Multi-Metric Optimization Using Generative Adversarial Networks for Near-End Speech Intelligibility Enhancement</a></li><li><a href=../../publication/zhang-21-asvspoof/>Multi-task Learning in Utterance-level and Segmental-level Spoof Detection</a></li><li><a href=../../publication/9711250/>OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild</a></li><li><a href=../../publication/williams-21-spsc/>Revisiting Speech Content Privacy</a></li><li><a href=../../publication/cooper-21-b-ssw/>Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis</a></li><li><a href=../../publication/sahidullah-2019/>Introduction to Voice Presentation Attack Detection and Recent Advances</a></li><li><a href=../../publication/tokuda-2017/>User Generated Dialogue Systems: uDialogue</a></li><li><a href=../../publication/evans-2014/>Speaker Recognition Anti-spoofing</a></li><li><a href=../../publication/creer-2010-building/>Building personalized synthetic voices for individuals with dysarthria using the HTS toolkit</a></li></ul></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Yamagishi Lab. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=../../js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=../../js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=../../en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script>
<script src=../../js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=../../js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>