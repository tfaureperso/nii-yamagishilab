---
title: Textless Phrase-Structure Induction from Visually-Grounded Speech

event: Invited talk
event_url: 

location: NII 1810
address:
  street: 
  city: 
  region: 
  postcode: 
  country: 

summary: Textless Phrase-Structure Induction from Visually-Grounded Speech
abstract: 'We study phrase structure induction from visually-grounded speech without intermediate text or text pre-trained models. The core idea is to first segment the speech waveform into sequences of word segments, then induce phrase structure based on the inferred segment-level continuous representations. To this end, we present the Audio-Visual Neural Syntax Learner (AV-NSL) that learns non-trivial phrase structure by listening to audio and looking at images, without ever reading text. Experiments on SpokenCOCO, the spoken version of MSCOCO with paired images and spoken captions, show that AV-NSL infers meaningful phrase structures similar to those learned from naturally-supervised text parsing, quantitatively and qualitatively. The findings in this paper extend prior work in unsupervised language acquisition from speech and grounded grammar induction, and manifest one possibility of bridging the gap between the two fields.
'

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: '2022-12-20T15:00:00Z'
date_end: '2030-06-01T16:00:00Z'
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: '2022-12-18T00:00:00Z'

authors: [Cheng-I Jeff Lai]
tags: []

# Is this a featured talk? (true/false)
featured: false

image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/bzdhc5b3Bxs)'
  focal_point: Right

url_code: ''
url_pdf: 'https://openreview.net/pdf?id=0c2SbGJ3Lt'
url_slides: ''
url_video: ''

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides:

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects:
---
