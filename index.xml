<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NII Yamagishi's Lab</title><link>https://zlin0.github.io/nii-yamagishilab/</link><atom:link href="https://zlin0.github.io/nii-yamagishilab/index.xml" rel="self" type="application/rss+xml"/><description>NII Yamagishi's Lab</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate><image><url>https://zlin0.github.io/nii-yamagishilab/media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_512x512_fill_lanczos_center_3.png</url><title>NII Yamagishi's Lab</title><link>https://zlin0.github.io/nii-yamagishilab/</link></image><item><title>Example Event</title><link>https://zlin0.github.io/nii-yamagishilab/talk/example-event/</link><pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talk/example-event/</guid><description>&lt;p>Slides can be added in a few ways:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Create&lt;/strong> slides using Wowchemy&amp;rsquo;s &lt;a href="https://wowchemy.com/docs/managing-content/#create-slides" target="_blank" rel="noopener">&lt;em>Slides&lt;/em>&lt;/a> feature and link using &lt;code>slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Upload&lt;/strong> an existing slide deck to &lt;code>static/&lt;/code> and link using &lt;code>url_slides&lt;/code> parameter in the front matter of the talk file&lt;/li>
&lt;li>&lt;strong>Embed&lt;/strong> your slides (e.g. Google Slides) or presentation video on this page using &lt;a href="https://wowchemy.com/docs/writing-markdown-latex/" target="_blank" rel="noopener">shortcodes&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Further event details, including page elements such as image galleries, can be added to the body of this page.&lt;/p></description></item><item><title>Cyber Vaccine for Deepfake Immunity</title><link>https://zlin0.github.io/nii-yamagishilab/publication/chang-2023-cyber/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/chang-2023-cyber/</guid><description/></item><item><title>Analysing the Masked Predictive Coding Training Criterion for Pre-Training a Speech Representation Model</title><link>https://zlin0.github.io/nii-yamagishilab/talks/upcoming/2023-0313-hemantyadav/</link><pubDate>Mon, 13 Mar 2023 16:30:00 +0900</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talks/upcoming/2023-0313-hemantyadav/</guid><description>&lt;h3 id="title">Title&lt;/h3>
&lt;p>Analysing the Masked Predictive Coding Training Criterion for Pre-Training a Speech Representation Model&lt;/p>
&lt;h3 id="abstract">Abstract&lt;/h3>
&lt;p>Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and cannot be controlled through the use of MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion&amp;rsquo;s that directly preserves both speaker and content information at various layers of a learnt model.&lt;/p>
&lt;h3 id="presenter-hemant-yadav">Presenter: Hemant Yadav&lt;/h3>
&lt;h3 id="location-nii-1512">Location: NII 1512&lt;/h3>
&lt;h3 id="time-2023-03-13-1630--1700">Time: 2023-03-13 16:30 ~ 17:00&lt;/h3>
&lt;h3 id="other-information">Other information:&lt;/h3>
&lt;p>Please email us to get the meeting link if you are intersted!&lt;/p></description></item><item><title>Enable Fact Verification across Languages via Parallel Training with Regularizations</title><link>https://zlin0.github.io/nii-yamagishilab/talks/past/2023-03-01-yc/</link><pubDate>Wed, 01 Mar 2023 11:00:00 +0900</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talks/past/2023-03-01-yc/</guid><description>&lt;h3 id="title">Title&lt;/h3>
&lt;p>Enable Fact Verification across Languages via Parallel Training with Regularizations&lt;/p>
&lt;h3 id="abstract">Abstract&lt;/h3>
&lt;p>Fact verification is part of the fact checking task. In this study, we want to enable the fact verification task across language. We will introduce a new cross-lingual fact verification dataset XFEVER, which is constructed by extending the examples of the processed FEVER dataset to 6 languages, including isolated language such as Japanese. Moreover, we apply two translation method, auto-translation and human-translation, to observe that our models&amp;rsquo; performance on different resources.
For evaluation, we will provide baselines in 2 scenarios: zero-shot transfer learning and translate-train learning. For the second scenario, we will introduce different consistency regularizations.&lt;/p>
&lt;h3 id="location-nii-1810">Location: NII 1810&lt;/h3>
&lt;h3 id="time-2023-03-01-1100--1200">Time: 2023-03-01 11:00 ~ 12:00&lt;/h3>
&lt;h3 id="other-information">Other information:&lt;/h3>
&lt;p>Please email us to get the meeting link if you are intersted!&lt;/p></description></item><item><title>Can Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems?</title><link>https://zlin0.github.io/nii-yamagishilab/publication/shi-2022-can/</link><pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/shi-2022-can/</guid><description/></item><item><title>Hiding speaker's sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline</title><link>https://zlin0.github.io/nii-yamagishilab/publication/noe-2022-hiding/</link><pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/noe-2022-hiding/</guid><description/></item><item><title>Joint Noise Reduction and Listening Enhancement for Full-End Speech Enhancement</title><link>https://zlin0.github.io/nii-yamagishilab/publication/li-2022-joint/</link><pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/li-2022-joint/</guid><description/></item><item><title>Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders</title><link>https://zlin0.github.io/nii-yamagishilab/publication/wang-2022-spoofed/</link><pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/wang-2022-spoofed/</guid><description/></item><item><title>Four papers were accepted to ICASSP2023!</title><link>https://zlin0.github.io/nii-yamagishilab/news/latest/2023-02-17-icassp/</link><pubDate>Thu, 16 Feb 2023 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/news/latest/2023-02-17-icassp/</guid><description>&lt;p>We have four papers were accepted to ICASSP2023:&lt;/p>
&lt;ul>
&lt;li>Paul-Gauthier Noé, XiaoXiao Miao, Xin Wang, Junichi Yamagishi, Jean-François Bonastre, Driss Matrouf(2022). &lt;a href="https://zlin0.github.io/nii-yamagishilab/publication/noe-2022-hiding/" target="_blank" rel="noopener">Hiding speaker&amp;rsquo;s sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline&lt;/a>&lt;/li>
&lt;li>Xuan Shi, Erica Cooper, Xin Wang, Junichi Yamagishi, Shrikanth Narayanan (2022). &lt;a href="https://zlin0.github.io/nii-yamagishilab/publication/shi-2022-can/" target="_blank" rel="noopener">Can Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems?&lt;/a>&lt;/li>
&lt;li>Xin Wang, Junichi Yamagishi (2022). &lt;a href="https://zlin0.github.io/nii-yamagishilab/publication/wang-2022-spoofed/" target="_blank" rel="noopener">Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders&lt;/a>&lt;/li>
&lt;li>Haoyu Li, Yun Liu, Junichi Yamagishi (2022). &lt;a href="https://zlin0.github.io/nii-yamagishilab/publication/li-2022-joint/" target="_blank" rel="noopener">Joint Noise Reduction and Listening Enhancement for Full-End Speech Enhancement&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Enabling Human-AI Co-Creativity in Music Making</title><link>https://zlin0.github.io/nii-yamagishilab/talks/past/2023-02-09-nicolas/</link><pubDate>Thu, 09 Feb 2023 17:00:00 +0900</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talks/past/2023-02-09-nicolas/</guid><description/></item><item><title>Few-shot Cross-speaker Style Transfer in Expressive Speech Synthesis</title><link>https://zlin0.github.io/nii-yamagishilab/talks/past/2023-02-07-gong/</link><pubDate>Tue, 07 Feb 2023 17:00:00 +0900</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talks/past/2023-02-07-gong/</guid><description/></item><item><title>Investigating Active-Learning-Based Training Data Selection for Speech Spoofing Countermeasure</title><link>https://zlin0.github.io/nii-yamagishilab/publication/wang-2022-investigating/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/wang-2022-investigating/</guid><description/></item><item><title>The PartialSpoof Database and Countermeasures for the Detection of Short Fake Speech Segments Embedded in an Utterance</title><link>https://zlin0.github.io/nii-yamagishilab/publication/zhang-2022-partial-spoof/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/zhang-2022-partial-spoof/</guid><description/></item><item><title>Textless Phrase-Structure Induction from Visually-Grounded Speech</title><link>https://zlin0.github.io/nii-yamagishilab/talks/past/2022-12-20-jeff/</link><pubDate>Tue, 20 Dec 2022 15:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talks/past/2022-12-20-jeff/</guid><description/></item><item><title>Predicting and synthesising plausible speech examples after oral cancer treatment</title><link>https://zlin0.github.io/nii-yamagishilab/talks/past/2022-12-21-bh/</link><pubDate>Tue, 20 Dec 2022 14:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talks/past/2022-12-21-bh/</guid><description/></item><item><title>Joint Speaker Encoder and Neural Back-end Model for Fully End-to-End Automatic Speaker Verification with Multiple Enrollment Utterances</title><link>https://zlin0.github.io/nii-yamagishilab/publication/zeng-2022-joint/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/zeng-2022-joint/</guid><description/></item><item><title>Outlier-Aware Training for Improving Group Accuracy Disparities</title><link>https://zlin0.github.io/nii-yamagishilab/publication/chen-2022-outlier/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/chen-2022-outlier/</guid><description/></item><item><title>We are recruiting postdocs!</title><link>https://zlin0.github.io/nii-yamagishilab/news/old/2022-11-02-recruite/</link><pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/news/old/2022-11-02-recruite/</guid><description>&lt;p>We are recruiting postdoctoral researchers in speech and audio processing!
&lt;a href="https://www.nii.ac.jp/en/about/recruit/2022/1007-2.html" target="_blank" rel="noopener">https://www.nii.ac.jp/en/about/recruit/2022/1007-2.html&lt;/a>&lt;/p>
&lt;p>We are also recruiting postdocs for topics in media processing, machine learning, multimedia security forensics, and social media!
&lt;a href="https://www.nii.ac.jp/en/about/recruit/2022/1028.html" target="_blank" rel="noopener">https://www.nii.ac.jp/en/about/recruit/2022/1028.html&lt;/a>&lt;/p></description></item><item><title>A Practical Guide to Logical Access Voice Presentation Attack Detection</title><link>https://zlin0.github.io/nii-yamagishilab/publication/wang-2022/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/wang-2022/</guid><description/></item><item><title>Analyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions</title><link>https://zlin0.github.io/nii-yamagishilab/publication/miao-22-interspeech/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/miao-22-interspeech/</guid><description/></item><item><title>ASVspoof</title><link>https://zlin0.github.io/nii-yamagishilab/challenges/asvspoof/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/challenges/asvspoof/</guid><description>&lt;h1 id="asvspoofhttpswwwasvspooforg">&lt;a href="https://www.asvspoof.org/" target="_blank" rel="noopener">ASVspoof&lt;/a>&lt;/h1>
&lt;h2 id="latest-information">Latest Information&lt;/h2>
&lt;p>Upcoming ASVspoof5 in 2023:&lt;/p>
&lt;ul>
&lt;li>Database creation: Autumn, 2022&lt;/li>
&lt;li>Challenge set-up: First half, 2023&lt;/li>
&lt;li>ASVspoof5 challenge: Second half, 2023&lt;/li>
&lt;/ul>
&lt;!--
## Introduction
## Organisers
(in alphabetical order) --></description></item><item><title>Attention Back-End for Automatic Speaker Verification with Multiple Enrollment Utterances</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9746688/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9746688/</guid><description/></item><item><title>Automatic Speaker Verification Spoofing and Deepfake Detection Using Wav2vec 2.0 and Data Augmentation</title><link>https://zlin0.github.io/nii-yamagishilab/publication/tak-22-odyssey/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/tak-22-odyssey/</guid><description/></item><item><title>Capsule-Forensics Networks for Deepfake Detection</title><link>https://zlin0.github.io/nii-yamagishilab/publication/nguyen-2022/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/nguyen-2022/</guid><description/></item><item><title>DDS: A new device-degraded speech dataset for speech enhancement</title><link>https://zlin0.github.io/nii-yamagishilab/publication/li-22-e-interspeech/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/li-22-e-interspeech/</guid><description/></item><item><title>Estimating the Confidence of Speech Spoofing Countermeasure</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9746204/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9746204/</guid><description/></item><item><title>Future Trends in Digital Face Manipulation and Detection</title><link>https://zlin0.github.io/nii-yamagishilab/publication/tolosana-2022/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/tolosana-2022/</guid><description/></item><item><title>Generalization Ability of MOS Prediction Networks</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9746395/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9746395/</guid><description/></item><item><title>Investigating Self-Supervised Front Ends for Speech Spoofing Countermeasures</title><link>https://zlin0.github.io/nii-yamagishilab/publication/wang-22-odyssey/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/wang-22-odyssey/</guid><description/></item><item><title>Language-Independent Speaker Anonymization Approach Using Self-Supervised Pre-Trained Models</title><link>https://zlin0.github.io/nii-yamagishilab/publication/miao-22-odyssey/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/miao-22-odyssey/</guid><description/></item><item><title>Master Face Attacks on Face Recognition Systems</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9758063/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9758063/</guid><description/></item><item><title>Mitigating the Diminishing Effect of Elastic Weight Consolidation</title><link>https://zlin0.github.io/nii-yamagishilab/publication/kruengkrai-2022-mitigating/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/kruengkrai-2022-mitigating/</guid><description/></item><item><title>On the Interplay between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9747728/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9747728/</guid><description/></item><item><title>Optimizing Tandem Speaker Verification and Anti-Spoofing Systems</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9664367/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9664367/</guid><description/></item><item><title>Robust Deepfake on Unrestricted Media: Generation and Detection</title><link>https://zlin0.github.io/nii-yamagishilab/publication/le-2022/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/le-2022/</guid><description/></item><item><title>Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022</title><link>https://zlin0.github.io/nii-yamagishilab/publication/zeng-22-interspeech/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/zeng-22-interspeech/</guid><description/></item><item><title>SVSNet: An End-to-End Speaker Voice Similarity Assessment Model</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9716822/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9716822/</guid><description/></item><item><title>The VoiceMOS Challenge 2022</title><link>https://zlin0.github.io/nii-yamagishilab/publication/huang-22-f-interspeech/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/huang-22-f-interspeech/</guid><description/></item><item><title>The VoicePrivacy 2020 Challenge: Results and findings</title><link>https://zlin0.github.io/nii-yamagishilab/publication/tomashenko-2022-voiceprivacy/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/tomashenko-2022-voiceprivacy/</guid><description/></item><item><title>Use of Speaker Recognition Approaches for Learning and Evaluating Embedding Representations of Musical Instrument Sounds</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9670718/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9670718/</guid><description/></item><item><title>VoiceMOS</title><link>https://zlin0.github.io/nii-yamagishilab/challenges/voicemos/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/challenges/voicemos/</guid><description>&lt;h1 id="voicemoshttpsvoicemos-challenge-2022githubio">&lt;a href="https://voicemos-challenge-2022.github.io/" target="_blank" rel="noopener">VoiceMOS&lt;/a>&lt;/h1>
&lt;p>The VoiceMOS Challenge 2022 has ended! Material from the challenge will remain available online. Read our summary paper of the challenge &lt;a href="https://arxiv.org/abs/2203.11389" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Human listening tests are the gold standard for evaluating synthesized speech. Objective measures of speech quality have low correlation with human ratings, and the generalization abilities of current data-driven quality prediction systems suffer significantly from domain mismatch. The VoiceMOS Challenge aims to encourage research in the area of automatic prediction of Mean Opinion Scores (MOS) for synthesized speech.&lt;/p>
&lt;h2 id="organisers">Organisers&lt;/h2>
&lt;ul>
&lt;li>Wen-Chin Huang (Nagoya University, Japan)&lt;/li>
&lt;li>Erica Cooper (National Institute of Informatics, Japan)&lt;/li>
&lt;li>Yu Tsao (Academia Sinica, Taiwan)&lt;/li>
&lt;li>Hsin-Min Wang (Academia Sinica, Taiwan)&lt;/li>
&lt;li>Tomoki Toda (Nagoya University, Japan)&lt;/li>
&lt;li>Junichi Yamagishi (National Institute of Informatics, Japan)&lt;/li>
&lt;/ul></description></item><item><title>VoicePrivacy</title><link>https://zlin0.github.io/nii-yamagishilab/challenges/voiceprivacy/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/challenges/voiceprivacy/</guid><description>&lt;h1 id="voiceprivacyhttpswwwvoiceprivacychallengeorg">&lt;a href="https://www.voiceprivacychallenge.org/" target="_blank" rel="noopener">VoicePrivacy&lt;/a>&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Formed in 2020, the VoicePrivacy initiative is spearheading the effort to develop privacy preservations solutions for speech technology. We aim to foster progress in the development of anonymisation and pseudonymisation solutions which suppress personally identifiable information contained within recordings of speech while preserving linguistic content and speech quality/naturalness. VoicePrivacy takes the form of a competitive benchmarking challenge, with common datasets, protocols and metrics. The first edition of VoicePrivacy was held in 2020, culminating in a special sessions held at INTERSPEECH 2020 and Odyssey 2020, and a special issue published in Elsevier Computer Speech and Language. VoicePrivacy 2022 is the second edition starts from March 2022.&lt;/p>
&lt;h2 id="organisers">Organisers&lt;/h2>
&lt;p>(in alphabetical order)&lt;/p>
&lt;ul>
&lt;li>Jean-François Bonastre - University of Avignon - LIA, France&lt;/li>
&lt;li>Pierre Champion - Inria, France&lt;/li>
&lt;li>Nicholas Evans - EURECOM, France&lt;/li>
&lt;li>Xiaoxiao Miao - NII, Japan&lt;/li>
&lt;li>Hubert Nourtel - Inria, France&lt;/li>
&lt;li>Massimiliano Todisco - EURECOM, France&lt;/li>
&lt;li>Natalia Tomashenko - University of Avignon - LIA, France&lt;/li>
&lt;li>Emmanuel Vincent - Inria, France&lt;/li>
&lt;li>Xin Wang - NII, Japan&lt;/li>
&lt;li>Junichi Yamagishi - NII, Japan and University of Edinburgh, UK&lt;/li>
&lt;/ul></description></item><item><title>A Multi-Level Attention Model for Evidence-Based Fact Checking</title><link>https://zlin0.github.io/nii-yamagishilab/publication/kruengkrai-etal-2021-multi/</link><pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/kruengkrai-etal-2021-multi/</guid><description/></item><item><title>A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection</title><link>https://zlin0.github.io/nii-yamagishilab/publication/wang-21-fa-interspeech/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/wang-21-fa-interspeech/</guid><description/></item><item><title>An Initial Investigation for Detecting Partially Spoofed Audio</title><link>https://zlin0.github.io/nii-yamagishilab/publication/zhang-21-ca-interspeech/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/zhang-21-ca-interspeech/</guid><description/></item><item><title>ASVspoof 2019: Spoofing Countermeasures for the Detection of Synthesized, Converted and Replayed Speech</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9358099/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9358099/</guid><description/></item><item><title>ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection</title><link>https://zlin0.github.io/nii-yamagishilab/publication/yamagishi-21-asvspoof/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/yamagishi-21-asvspoof/</guid><description/></item><item><title>Benchmarking and challenges in security and privacy for voice biometrics</title><link>https://zlin0.github.io/nii-yamagishilab/publication/bonastre-21-spsc/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/bonastre-21-spsc/</guid><description/></item><item><title>Denoising-and-Dereverberation Hierarchical Neural Vocoder for Robust Waveform Generation</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9383611/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9383611/</guid><description/></item><item><title>Effectiveness of Detection-based and Regression-based Approaches for Estimating Mask-Wearing Ratio</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9667046/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9667046/</guid><description/></item><item><title>End-to-End Text-to-Speech Using Latent Duration Based on VQ-VAE</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9414499/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9414499/</guid><description/></item><item><title>Enhancing Low-Quality Voice Recordings Using Disentangled Channel Factor and Neural Waveform Model</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9383507/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9383507/</guid><description/></item><item><title>Exploring Disentanglement with Multilingual and Monolingual VQ-VAE</title><link>https://zlin0.github.io/nii-yamagishilab/publication/williams-21-ssw/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/williams-21-ssw/</guid><description/></item><item><title>How do Voices from Past Speech Synthesis Challenges Compare Today?</title><link>https://zlin0.github.io/nii-yamagishilab/publication/cooper-21-ssw/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/cooper-21-ssw/</guid><description/></item><item><title>How Similar or Different is Rakugo Speech Synthesizer to Professional Performers?</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9414175/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9414175/</guid><description/></item><item><title>Learning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9413543/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9413543/</guid><description/></item><item><title>Multi-Metric Optimization Using Generative Adversarial Networks for Near-End Speech Intelligibility Enhancement</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9536406/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9536406/</guid><description/></item><item><title>Multi-task Learning in Utterance-level and Segmental-level Spoof Detection</title><link>https://zlin0.github.io/nii-yamagishilab/publication/zhang-21-asvspoof/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/zhang-21-asvspoof/</guid><description/></item><item><title>OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild</title><link>https://zlin0.github.io/nii-yamagishilab/publication/9711250/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/9711250/</guid><description/></item><item><title>Revisiting Speech Content Privacy</title><link>https://zlin0.github.io/nii-yamagishilab/publication/williams-21-spsc/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/williams-21-spsc/</guid><description/></item><item><title>Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis</title><link>https://zlin0.github.io/nii-yamagishilab/publication/cooper-21-b-ssw/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/cooper-21-b-ssw/</guid><description/></item><item><title>Introduction to Voice Presentation Attack Detection and Recent Advances</title><link>https://zlin0.github.io/nii-yamagishilab/publication/sahidullah-2019/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/sahidullah-2019/</guid><description/></item><item><title>User Generated Dialogue Systems: uDialogue</title><link>https://zlin0.github.io/nii-yamagishilab/publication/tokuda-2017/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/tokuda-2017/</guid><description/></item><item><title>おしゃべりなコンピュータ : 音声合成技術の現在と未来</title><link>https://zlin0.github.io/nii-yamagishilab/publication/bb-18335564/</link><pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/bb-18335564/</guid><description/></item><item><title>Speaker Recognition Anti-spoofing</title><link>https://zlin0.github.io/nii-yamagishilab/publication/evans-2014/</link><pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/evans-2014/</guid><description/></item><item><title>Building personalized synthetic voices for individuals with dysarthria using the HTS toolkit</title><link>https://zlin0.github.io/nii-yamagishilab/publication/creer-2010-building/</link><pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/publication/creer-2010-building/</guid><description/></item><item><title/><link>https://zlin0.github.io/nii-yamagishilab/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/admin/config.yml</guid><description/></item><item><title/><link>https://zlin0.github.io/nii-yamagishilab/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/contact/</guid><description/></item><item><title/><link>https://zlin0.github.io/nii-yamagishilab/databases/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/databases/</guid><description/></item><item><title/><link>https://zlin0.github.io/nii-yamagishilab/grants/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/grants/</guid><description/></item><item><title/><link>https://zlin0.github.io/nii-yamagishilab/people/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/people/</guid><description/></item><item><title/><link>https://zlin0.github.io/nii-yamagishilab/recruitments/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/recruitments/</guid><description/></item><item><title>Tour</title><link>https://zlin0.github.io/nii-yamagishilab/tour/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/tour/</guid><description/></item></channel></rss>