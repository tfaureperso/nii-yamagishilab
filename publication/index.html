<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=stylesheet href=/nii-yamagishilab/css/vendor-bundle.min.16f785cdb553c8c4431db6775122af35.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/nii-yamagishilab/css/wowchemy.f3c28803eb07089619aac681dc14ff29.css><link rel=stylesheet href=/nii-yamagishilab/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/nii-yamagishilab/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Junichi Yamagishi"><meta name=description content="Professor"><link rel=alternate hreflang=en-us href=https://zlin0.github.io/nii-yamagishilab/publication/><link rel=canonical href=https://zlin0.github.io/nii-yamagishilab/publication/><link rel=manifest href=/nii-yamagishilab/manifest.webmanifest><link rel=icon type=image/png href=/nii-yamagishilab/media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/nii-yamagishilab/media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#3f51b5"><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@yamagishilab"><meta property="twitter:creator" content="@yamagishilab"><meta property="twitter:image" content="https://zlin0.github.io/nii-yamagishilab/media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_512x512_fill_lanczos_center_3.png"><meta property="og:site_name" content="NII Yamagishi's Lab"><meta property="og:url" content="https://zlin0.github.io/nii-yamagishilab/publication/"><meta property="og:title" content="Publications | NII Yamagishi's Lab"><meta property="og:description" content="Professor"><meta property="og:image" content="https://zlin0.github.io/nii-yamagishilab/media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-02-18T00:00:00+00:00"><link rel=alternate href=/nii-yamagishilab/publication/index.xml type=application/rss+xml title="NII Yamagishi's Lab"><title>Publications | NII Yamagishi's Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=3a079e7dad19be978a318345a7749d34><script src=/nii-yamagishilab/js/wowchemy-init.min.fe8634e7d00f14d07fb33caf14cc8e55.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>NII Yamagishi's Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>NII Yamagishi's Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>News</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/nii-yamagishilab/news/latest/><span>Latest</span></a>
<a class=dropdown-item href=/nii-yamagishilab/news/old/><span>Old</span></a></div></li><li class=nav-item><a class=nav-link href=/nii-yamagishilab/people><span>Members</span></a></li><li class=nav-item><a class=nav-link href=/nii-yamagishilab/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/nii-yamagishilab/challenges><span>Challenges</span></a></li><li class=nav-item><a class=nav-link href=/nii-yamagishilab/databases><span>Databases</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Talks</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/nii-yamagishilab/talks/upcoming/><span>Upcoming</span></a>
<a class=dropdown-item href=/nii-yamagishilab/talks/past/><span>Past</span></a></div></li><li class=nav-item><a class=nav-link href=/nii-yamagishilab/grants><span>Grants</span></a></li><li class=nav-item><a class=nav-link href=/nii-yamagishilab/recruitments><span>Recruitments</span></a></li><li class=nav-item><a class=nav-link href=/nii-yamagishilab/contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Publications</h1></div><div class=universal-wrapper><div class=row><div class=col-lg-12><div class="form-row mb-4"><div class=col-auto><input type=search class="filter-search form-control form-control-sm" placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off role=textbox spellcheck=false></div><div class=col-auto><select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group=pubtype><option value=*>Type</option><option value=.pubtype-1>
Conference paper</option><option value=.pubtype-2>
Journal article</option><option value=.pubtype-5>
Book</option><option value=.pubtype-6>
Book section</option></select></div><div class=col-auto><select class="pub-filters form-control form-control-sm" data-filter-group=year><option value=*>Date</option><option value=.year-2023>
2023</option><option value=.year-2022>
2022</option><option value=.year-2021>
2021</option><option value=.year-2019>
2019</option><option value=.year-2017>
2017</option><option value=.year-2015>
2015</option><option value=.year-2014>
2014</option><option value=.year-2010>
2010</option></select></div></div><div id=container-publications><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2023).
<a href=/nii-yamagishilab/publication/wang-2022-spoofed/>Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders</a>.
<em>Accepted by ICASSP 2023</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/wang-2022-spoofed/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2210.10570 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/haoyu-li/>Haoyu Li</a></span>, <span><a href=/nii-yamagishilab/author/yun-liu/>Yun Liu</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2023).
<a href=/nii-yamagishilab/publication/li-2022-joint/>Joint Noise Reduction and Listening Enhancement for Full-End Speech Enhancement</a>.
<em>Accepted by ICASSP2023</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/li-2022-joint/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.11500 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/paul-gauthier-noe/>‪Paul-Gauthier Noé‬</a></span>, <span><a href=/nii-yamagishilab/author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/jean-francois-bonastre/>Jean-François Bonastre</a></span>, <span><a href=/nii-yamagishilab/author/driss-matrouf/>Driss Matrouf</a></span></span>
(2023).
<a href=/nii-yamagishilab/publication/noe-2022-hiding/>Hiding speaker's sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline</a>.
<em>Accepted by ICASSP2023</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/noe-2022-hiding/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2211.16065 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/speaker_sex_attribute_privacy target=_blank rel=noopener>Codes and samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://zenodo.org/record/7347685 target=_blank rel=noopener>Pre-trained models</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xuan-shi/>Xuan Shi</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/shrikanth-narayanan/>Shrikanth Narayanan</a></span></span>
(2023).
<a href=/nii-yamagishilab/publication/shi-2022-can/>Can Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems?</a>.
<em>Accepted by ICASSP 2023</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/shi-2022-can/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=hhttps://arxiv.org/abs/2211.13868>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/sample-midi-to-audio/ target=_blank rel=noopener>Samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://zenodo.org/record/7370009 target=_blank rel=noopener>Pre-trained models</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2023"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/lin-zhang/>Lin Zhang</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2023).
<a href=/nii-yamagishilab/publication/zhang-2022-partial-spoof/>The PartialSpoof Database and Countermeasures for the Detection of Short Fake Speech Segments Embedded in an Utterance</a>.
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/zhang-2022-partial-spoof/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TASLP.2022.3233236 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2204.05177 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2023).
<a href=/nii-yamagishilab/publication/wang-2022-investigating/>Investigating Active-Learning-Based Training Data Selection for Speech Spoofing Countermeasure</a>.
<em>2022 IEEE Spoken Language Technology Workshop (SLT)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/wang-2022-investigating/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/SLT54892.2023.10023350 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.14553 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/li-kuang-chen/>Li-Kuang Chen</a></span>, <span><a href=/nii-yamagishilab/author/canasai-kruengkrai/>Canasai Kruengkrai</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/chen-2022-outlier/>Outlier-Aware Training for Improving Group Accuracy Disparities</a>.
<em>Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Student Research Workshop</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.aacl-srw.8 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/chen-2022-outlier/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2210.15183 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/jtt-m target=_blank rel=noopener>Codes</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.7260028 target=_blank rel=noopener>Pre-trained models</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/chang-zeng/>Chang Zeng</a></span>, <span><a href=/nii-yamagishilab/author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/zeng-2022-joint/>Joint Speaker Encoder and Neural Back-end Model for Fully End-to-End Automatic Speaker Verification with Multiple Enrollment Utterances</a>.
<em>Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/zeng-2022-joint/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2209.00485 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xuan-shi/>Xuan Shi</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/9670718/>Use of Speaker Recognition Approaches for Learning and Evaluating Embedding Representations of Musical Instrument Sounds</a>.
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9670718/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TASLP.2022.3140549 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/natalia-tomashenko/>Natalia Tomashenko</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/emmanuel-vincent/>Emmanuel Vincent</a></span>, <span><a href=/nii-yamagishilab/author/jose-patino/>Jose Patino</a></span>, <span><a href=/nii-yamagishilab/author/brij-mohan-lal-srivastava/>Brij Mohan Lal Srivastava</a></span>, <span><a href=/nii-yamagishilab/author/paul-gauthier-noe/>‪Paul-Gauthier Noé‬</a></span>, <span><a href=/nii-yamagishilab/author/andreas-nautsch/>Andreas Nautsch</a></span>, <span><a href=/nii-yamagishilab/author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/benjamin-obrien/>Benjamin O’Brien</a></span>, <span><a href=/nii-yamagishilab/author/others/>others</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/tomashenko-2022-voiceprivacy/>The VoicePrivacy 2020 Challenge: Results and findings</a>.
<em>Computer Speech & Language</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/tomashenko-2022-voiceprivacy/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1016/j.csl.2022.101362 target=_blank rel=noopener>Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2109.00648 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.voiceprivacychallenge.org/ target=_blank rel=noopener>challenge website</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/wen-chin-huang/>Wen-Chin Huang</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/yu-tsao/>Yu Tsao</a></span>, <span><a href=/nii-yamagishilab/author/hsin-min-wang/>Hsin-Min Wang</a></span>, <span><a href=/nii-yamagishilab/author/tomoki-toda/>Tomoki Toda</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/huang-22-f-interspeech/>The VoiceMOS Challenge 2022</a>.
<em>Proc. Interspeech 2022</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/huang-22-f-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2022-970 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.11389 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://codalab.lisn.upsaclay.fr/competitions/695 target=_blank rel=noopener>CodaLab</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://voicemos-challenge-2022.github.io/ target=_blank rel=noopener>website</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/cheng-hung-hu/>Cheng-Hung Hu</a></span>, <span><a href=/nii-yamagishilab/author/yu-huai-peng/>Yu-Huai Peng</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/yu-tsao/>Yu Tsao</a></span>, <span><a href=/nii-yamagishilab/author/hsin-min-wang/>Hsin-Min Wang</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/9716822/>SVSNet: An End-to-End Speaker Voice Similarity Assessment Model</a>.
<em>IEEE Signal Processing Letters</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9716822/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/LSP.2022.3152672 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2107.09392 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/chang-zeng/>Chang Zeng</a></span>, <span><a href=/nii-yamagishilab/author/lin-zhang/>Lin Zhang</a></span>, <span><a href=/nii-yamagishilab/author/meng-liu/>Meng Liu</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/zeng-22-interspeech/>Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022</a>.
<em>Proc. Interspeech 2022</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/zeng-22-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2022-10495 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2209.00423 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/trung-nghia-le/>Trung-Nghia Le</a></span>, <span><a href=/nii-yamagishilab/author/huy-hong-nguyen/>Huy Hong Nguyen</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/isao-echizen/>Isao Echizen</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/le-2022/>Robust Deepfake on Unrestricted Media: Generation and Detection</a>.
<em>Frontiers in Fake Media Generation and Detection</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-981-19-1524-6_4 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/le-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-981-19-1524-6_4 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/anssi-kanervisto/>Anssi Kanervisto</a></span>, <span><a href=/nii-yamagishilab/author/ville-hautamaki/>Ville Hautamäki</a></span>, <span><a href=/nii-yamagishilab/author/tomi-kinnunen/>Tomi Kinnunen</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/9664367/>Optimizing Tandem Speaker Verification and Anti-Spoofing Systems</a>.
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9664367/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TASLP.2021.3138681 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/cheng-i-jeff-lai/>Cheng-I Jeff Lai</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/yang-zhang/>Yang Zhang</a></span>, <span><a href=/nii-yamagishilab/author/shiyu-chang/>Shiyu Chang</a></span>, <span><a href=/nii-yamagishilab/author/kaizhi-qian/>Kaizhi Qian</a></span>, <span><a href=/nii-yamagishilab/author/yi-lun-liao/>Yi-Lun Liao</a></span>, <span><a href=/nii-yamagishilab/author/yung-sung-chuang/>Yung-Sung Chuang</a></span>, <span><a href=/nii-yamagishilab/author/alexander-h.-liu/>Alexander H. Liu</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/david-cox/>David Cox</a></span>, <span><a href=/nii-yamagishilab/author/james-glass/>James Glass</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/9747728/>On the Interplay between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis</a>.
<em>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9747728/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP43922.2022.9747728 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.01147 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://people.csail.mit.edu/clai24/prune-tts/ target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/canasai-kruengkrai/>Canasai Kruengkrai</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/kruengkrai-2022-mitigating/>Mitigating the Diminishing Effect of Elastic Weight Consolidation</a>.
<em>Proceedings of the 29th International Conference on Computational Linguistics</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/kruengkrai-2022-mitigating/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.coling-1.403.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/ewc target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/huy-hong-nguyen/>Huy Hong Nguyen</a></span>, <span><a href=/nii-yamagishilab/author/sebastien-marcel/>Sebastien Marcel</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/isao-echizen/>Isao Echizen</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/9758063/>Master Face Attacks on Face Recognition Systems</a>.
<em>IEEE Transactions on Biometrics, Behavior, and Identity Science</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9758063/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TBIOM.2022.3166206 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/natalia-tomashenko/>Natalia Tomashenko</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/miao-22-odyssey/>Language-Independent Speaker Anonymization Approach Using Self-Supervised Pre-Trained Models</a>.
<em>Proc. The Speaker and Language Recognition Workshop (Odyssey 2022)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/miao-22-odyssey/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Odyssey.2022-39 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2202.13097 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/SAS-audio-samples/ target=_blank rel=noopener>Samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/SSL-SAS target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/wang-22-odyssey/>Investigating Self-Supervised Front Ends for Speech Spoofing Countermeasures</a>.
<em>Proc. The Speaker and Language Recognition Workshop (Odyssey 2022)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/wang-22-odyssey/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Odyssey.2022-14 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2111.07725 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/wen-chin-huang/>Wen-Chin Huang</a></span>, <span><a href=/nii-yamagishilab/author/tomoki-toda/>Tomoki Toda</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/9746395/>Generalization Ability of MOS Prediction Networks</a>.
<em>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9746395/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP43922.2022.9746395 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.02635 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/mos-finetune-ssl target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/ruben-tolosana/>Ruben Tolosana</a></span>, <span><a href=/nii-yamagishilab/author/christian-rathgeb/>Christian Rathgeb</a></span>, <span><a href=/nii-yamagishilab/author/ruben-vera-rodriguez/>Ruben Vera-Rodriguez</a></span>, <span><a href=/nii-yamagishilab/author/christoph-busch/>Christoph Busch</a></span>, <span><a href=/nii-yamagishilab/author/luisa-verdoliva/>Luisa Verdoliva</a></span>, <span><a href=/nii-yamagishilab/author/siwei-lyu/>Siwei Lyu</a></span>, <span><a href=/nii-yamagishilab/author/huy-hong-nguyen/>Huy Hong Nguyen</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/isao-echizen/>Isao Echizen</a></span>, <span><a href=/nii-yamagishilab/author/peter-rot/>Peter Rot</a></span>, <span><a href=/nii-yamagishilab/author/klemen-grm/>Klemen Grm</a></span>, <span><a href=/nii-yamagishilab/author/vitomir-struc/>Vitomir Štruc</a></span>, <span><a href=/nii-yamagishilab/author/antitza-dantcheva/>Antitza Dantcheva</a></span>, <span><a href=/nii-yamagishilab/author/zahid-akhtar/>Zahid Akhtar</a></span>, <span><a href=/nii-yamagishilab/author/sergio-romero-tapiador/>Sergio Romero-Tapiador</a></span>, <span><a href=/nii-yamagishilab/author/julian-fierrez/>Julian Fierrez</a></span>, <span><a href=/nii-yamagishilab/author/aythami-morales/>Aythami Morales</a></span>, <span><a href=/nii-yamagishilab/author/javier-ortega-garcia/>Javier Ortega-Garcia</a></span>, <span><a href=/nii-yamagishilab/author/els-kindt/>Els Kindt</a></span>, <span><a href=/nii-yamagishilab/author/catherine-jasserand/>Catherine Jasserand</a></span>, <span><a href=/nii-yamagishilab/author/tarmo-kalvet/>Tarmo Kalvet</a></span>, <span><a href=/nii-yamagishilab/author/marek-tiits/>Marek Tiits</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/tolosana-2022/>Future Trends in Digital Face Manipulation and Detection</a>.
<em>Handbook of Digital Face Manipulation and Detection: From DeepFakes to Morphing Attacks</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-87664-7_21 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/tolosana-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-87664-7_21 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/9746204/>Estimating the Confidence of Speech Spoofing Countermeasure</a>.
<em>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9746204/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP43922.2022.9746204 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.04775 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/haoyu-li/>Haoyu Li</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/li-22-e-interspeech/>DDS: A new device-degraded speech dataset for speech enhancement</a>.
<em>Proc. Interspeech 2022</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/li-22-e-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2022-441 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2109.07931 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.5464104 target=_blank rel=noopener>Database DDS database (DAPS portion)</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.5499506 target=_blank rel=noopener>DDS database (VCTK portion part 1)</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.5501697 target=_blank rel=noopener>DDS database (VCTK portion part 2)</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/huy-hong-nguyen/>Huy Hong Nguyen</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/isao-echizen/>Isao Echizen</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/nguyen-2022/>Capsule-Forensics Networks for Deepfake Detection</a>.
<em>Handbook of Digital Face Manipulation and Detection: From DeepFakes to Morphing Attacks</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-87664-7_13 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/nguyen-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-87664-7_13 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/hemlata-tak/>Hemlata Tak</a></span>, <span><a href=/nii-yamagishilab/author/massimiliano-todisco/>Massimiliano Todisco</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/jee-weon-jung/>Jee-weon Jung</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/nicholas-evans/>Nicholas Evans</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/tak-22-odyssey/>Automatic Speaker Verification Spoofing and Deepfake Detection Using Wav2vec 2.0 and Data Augmentation</a>.
<em>Proc. The Speaker and Language Recognition Workshop (Odyssey 2022)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/tak-22-odyssey/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Odyssey.2022-16 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2202.12233 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/chang-zeng/>Chang Zeng</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/9746688/>Attention Back-End for Automatic Speaker Verification with Multiple Enrollment Utterances</a>.
<em>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9746688/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP43922.2022.9746688 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2104.01541v2 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/Attention_Backend_for_ASV target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/natalia-tomashenko/>Natalia Tomashenko</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/miao-22-interspeech/>Analyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions</a>.
<em>Proc. Interspeech 2022</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/miao-22-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2022-11065 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.14834 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/SAS-audio-samples/ target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=/nii-yamagishilab/publication/wang-2022/>A Practical Guide to Logical Access Voice Presentation Attack Detection</a>.
<em>Frontiers in Fake Media Generation and Detection</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-981-19-1524-6_8 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/wang-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-981-19-1524-6_8 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/canasai-kruengkrai/>Canasai Kruengkrai</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/kruengkrai-etal-2021-multi/>A Multi-Level Attention Model for Evidence-Based Fact Checking</a>.
<em>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/kruengkrai-etal-2021-multi/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2021.findings-acl.217 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2021.findings-acl.217 target=_blank rel=noopener>URL</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/cooper-21-b-ssw/>Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis</a>.
<em>Proc. 11th ISCA Speech Synthesis Workshop (SSW 11)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/cooper-21-b-ssw/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SSW.2021-23 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2104.12292 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/samples-xin/main-midi2audio.html target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/jennifer-williams/>Jennifer Williams</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/paul-gauthier/>Paul-Gauthier</a></span>, <span><a href=/nii-yamagishilab/author/cassia-valentini-botinhao/>Cassia Valentini-Botinhao</a></span>, <span><a href=/nii-yamagishilab/author/jean-francois-bonastre/>Jean-François Bonastre</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/williams-21-spsc/>Revisiting Speech Content Privacy</a>.
<em>Proc. 2021 ISCA Symposium on Security and Privacy in Speech Communication</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/williams-21-spsc/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SPSC.2021-9 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.06760 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/trung-nghia-le/>Trung-Nghia Le</a></span>, <span><a href=/nii-yamagishilab/author/huy-hong-nguyen/>Huy Hong Nguyen</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/isao-echizen/>Isao Echizen</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/9711250/>OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild</a>.
<em>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9711250/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICCV48922.2021.00996 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2107.14480 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.5528418 target=_blank rel=noopener>Database</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/lin-zhang/>Lin Zhang</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/zhang-21-asvspoof/>Multi-task Learning in Utterance-level and Segmental-level Spoof Detection</a>.
<em>Proc. 2021 Edition of the Automatic Speaker Verification and Spoofing Countermeasures Challenge</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/zhang-21-asvspoof/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/ASVSPOOF.2021-2 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2107.14132 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://zenodo.org/record/5112031 target=_blank rel=noopener>Database</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/haoyu-li/>Haoyu Li</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/9536406/>Multi-Metric Optimization Using Generative Adversarial Networks for Near-End Speech Intelligibility Enhancement</a>.
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9536406/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TASLP.2021.3111566 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2104.08499 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/hyli666-demos/intelligibility/index.html target=_blank rel=noopener>Samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/NELE-GAN target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/jennifer-williams/>Jennifer Williams</a></span>, <span><a href=/nii-yamagishilab/author/yi-zhao/>Yi Zhao</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/9413543/>Learning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm</a>.
<em>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9413543/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP39728.2021.9413543 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2010.10727 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://rhoposit.github.io/icassp2021 target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/shuhei-kato/>Shuhei Kato</a></span>, <span><a href=/nii-yamagishilab/author/yusuke-yasuda/>Yusuke Yasuda</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/9414175/>How Similar or Different is Rakugo Speech Synthesizer to Professional Performers?</a>.
<em>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9414175/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP39728.2021.9414175 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2010.11549 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/cooper-21-ssw/>How do Voices from Past Speech Synthesis Challenges Compare Today?</a>.
<em>Proc. 11th ISCA Speech Synthesis Workshop (SSW 11)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/cooper-21-ssw/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SSW.2021-32 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2105.02373 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/jennifer-williams/>Jennifer Williams</a></span>, <span><a href=/nii-yamagishilab/author/jason-fong/>Jason Fong</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/williams-21-ssw/>Exploring Disentanglement with Multilingual and Monolingual VQ-VAE</a>.
<em>Proc. 11th ISCA Speech Synthesis Workshop (SSW 11)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/williams-21-ssw/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SSW.2021-22 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2105.01573 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://rhoposit.github.io/ssw11 target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/haoyu-li/>Haoyu Li</a></span>, <span><a href=/nii-yamagishilab/author/yang-ai/>Yang Ai</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/9383507/>Enhancing Low-Quality Voice Recordings Using Disentangled Channel Factor and Neural Waveform Model</a>.
<em>2021 IEEE Spoken Language Technology Workshop (SLT)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9383507/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/SLT48900.2021.9383507 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2011.05038 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/hyli666-demos/evr-slt2021 target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/yusuke-yasuda/>Yusuke Yasuda</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishd/>Junichi Yamagishd</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/9414499/>End-to-End Text-to-Speech Using Latent Duration Based on VQ-VAE</a>.
<em>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9414499/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP39728.2021.9414499 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2010.09602 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/sample-tts-latent-duration/ target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/khanh-duv-nguyen/>Khanh-Duv Nguyen</a></span>, <span><a href=/nii-yamagishilab/author/huv-h.-nguyen/>Huv H. Nguyen</a></span>, <span><a href=/nii-yamagishilab/author/trung-nghia-le/>Trung-Nghia Le</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/isao-echizen/>Isao Echizen</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/9667046/>Effectiveness of Detection-based and Regression-based Approaches for Estimating Mask-Wearing Ratio</a>.
<em>2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9667046/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/FG52635.2021.9667046 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2111.12888 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/yang-ai/>Yang Ai</a></span>, <span><a href=/nii-yamagishilab/author/haoyu-li/>Haoyu Li</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/zhenhua-ling/>Zhenhua Ling</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/9383611/>Denoising-and-Dereverberation Hierarchical Neural Vocoder for Robust Waveform Generation</a>.
<em>2021 IEEE Spoken Language Technology Workshop (SLT)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9383611/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/SLT48900.2021.9383611 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2011.03955 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=http://home.ustc.edu.cn/~ay8067/DNR/demo.html target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/jean-francois-bonastre/>Jean-François Bonastre</a></span>, <span><a href=/nii-yamagishilab/author/hector-delgado/>Héctor Delgado</a></span>, <span><a href=/nii-yamagishilab/author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=/nii-yamagishilab/author/tomi-kinnunen/>Tomi Kinnunen</a></span>, <span><a href=/nii-yamagishilab/author/kong-aik-lee/>Kong Aik Lee</a></span>, <span><a href=/nii-yamagishilab/author/xuechen-liu/>Xuechen Liu</a></span>, <span><a href=/nii-yamagishilab/author/andreas-nautsch/>Andreas Nautsch</a></span>, <span><a href=/nii-yamagishilab/author/paul-gauthier-noe/>‪Paul-Gauthier Noé‬</a></span>, <span><a href=/nii-yamagishilab/author/jose-patino/>Jose Patino</a></span>, <span><a href=/nii-yamagishilab/author/md-sahidullah/>Md Sahidullah</a></span>, <span><a href=/nii-yamagishilab/author/brij-mohan-lal-srivastava/>Brij Mohan Lal Srivastava</a></span>, <span><a href=/nii-yamagishilab/author/massimiliano-todisco/>Massimiliano Todisco</a></span>, <span><a href=/nii-yamagishilab/author/natalia-tomashenko/>Natalia Tomashenko</a></span>, <span><a href=/nii-yamagishilab/author/emmanuel-vincent/>Emmanuel Vincent</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/bonastre-21-spsc/>Benchmarking and challenges in security and privacy for voice biometrics</a>.
<em>Proc. 2021 ISCA Symposium on Security and Privacy in Speech Communication</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/bonastre-21-spsc/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SPSC.2021-11 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2109.00281 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/massimiliano-todisco/>Massimiliano Todisco</a></span>, <span><a href=/nii-yamagishilab/author/md-sahidullah/>Md Sahidullah</a></span>, <span><a href=/nii-yamagishilab/author/jose-patino/>Jose Patino</a></span>, <span><a href=/nii-yamagishilab/author/andreas-nautsch/>Andreas Nautsch</a></span>, <span><a href=/nii-yamagishilab/author/xuechen-liu/>Xuechen Liu</a></span>, <span><a href=/nii-yamagishilab/author/kong-aik-lee/>Kong Aik Lee</a></span>, <span><a href=/nii-yamagishilab/author/tomi-kinnunen/>Tomi Kinnunen</a></span>, <span><a href=/nii-yamagishilab/author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=/nii-yamagishilab/author/hector-delgado/>Héctor Delgado</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/yamagishi-21-asvspoof/>ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection</a>.
<em>Proc. 2021 Edition of the Automatic Speaker Verification and Spoofing Countermeasures Challenge</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/yamagishi-21-asvspoof/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/ASVSPOOF.2021-8 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2109.00537 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.asvspoof.org/ target=_blank rel=noopener>challenge website</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/andreas-nautsch/>Andreas Nautsch</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=/nii-yamagishilab/author/tomi-h.-kinnunen/>Tomi H. Kinnunen</a></span>, <span><a href=/nii-yamagishilab/author/ville-vestman/>Ville Vestman</a></span>, <span><a href=/nii-yamagishilab/author/massimiliano-todisco/>Massimiliano Todisco</a></span>, <span><a href=/nii-yamagishilab/author/hector-delgado/>Héctor Delgado</a></span>, <span><a href=/nii-yamagishilab/author/md-sahidullah/>Md Sahidullah</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/kong-aik-lee/>Kong Aik Lee</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/9358099/>ASVspoof 2019: Spoofing Countermeasures for the Detection of Synthesized, Converted and Replayed Speech</a>.
<em>IEEE Transactions on Biometrics, Behavior, and Identity Science</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/9358099/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TBIOM.2021.3059479 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2102.05889 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/lin-zhang/>Lin Zhang</a></span>, <span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/erica-cooper/>Erica Cooper</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/jose-patino/>Jose Patino</a></span>, <span><a href=/nii-yamagishilab/author/nicholas-evans/>Nicholas Evans</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/zhang-21-ca-interspeech/>An Initial Investigation for Detecting Partially Spoofed Audio</a>.
<em>Proc. Interspeech 2021</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/zhang-21-ca-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2021-738 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2104.02518 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/zlin-demo/IS2021/index.html target=_blank rel=noopener>Samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.4817532 target=_blank rel=noopener>Database</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/xin-wang/>Xin Wang</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=/nii-yamagishilab/publication/wang-21-fa-interspeech/>A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection</a>.
<em>Proc. Interspeech 2021</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/wang-21-fa-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2021-702 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2103.11326 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2019"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/md-sahidullah/>Md Sahidullah</a></span>, <span><a href=/nii-yamagishilab/author/hector-delgado/>Héctor Delgado</a></span>, <span><a href=/nii-yamagishilab/author/massimiliano-todisco/>Massimiliano Todisco</a></span>, <span><a href=/nii-yamagishilab/author/tomi-kinnunen/>Tomi Kinnunen</a></span>, <span><a href=/nii-yamagishilab/author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/kong-aik-lee/>Kong Aik Lee</a></span></span>
(2019).
<a href=/nii-yamagishilab/publication/sahidullah-2019/>Introduction to Voice Presentation Attack Detection and Recent Advances</a>.
<em>Handbook of Biometric Anti-Spoofing: Presentation Attack Detection</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-319-92627-8_15 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/sahidullah-2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-319-92627-8_15 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2017"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/keiichi-tokuda/>Keiichi Tokuda</a></span>, <span><a href=/nii-yamagishilab/author/akinobu-lee/>Akinobu Lee</a></span>, <span><a href=/nii-yamagishilab/author/yoshihiko-nankaku/>Yoshihiko Nankaku</a></span>, <span><a href=/nii-yamagishilab/author/keiichiro-oura/>Keiichiro Oura</a></span>, <span><a href=/nii-yamagishilab/author/kei-hashimoto/>Kei Hashimoto</a></span>, <span><a href=/nii-yamagishilab/author/daisuke-yamamoto/>Daisuke Yamamoto</a></span>, <span><a href=/nii-yamagishilab/author/ichi-takumi/>Ichi Takumi</a></span>, <span><a href=/nii-yamagishilab/author/takahiro-uchiya/>Takahiro Uchiya</a></span>, <span><a href=/nii-yamagishilab/author/shuhei-tsutsumi/>Shuhei Tsutsumi</a></span>, <span><a href=/nii-yamagishilab/author/steve-renals/>Steve Renals</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2017).
<a href=/nii-yamagishilab/publication/tokuda-2017/>User Generated Dialogue Systems: uDialogue</a>.
<em>Human-Harmonized Information Technology, Volume 2: Horizontal Expansion</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-4-431-56535-2_3 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/tokuda-2017/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-4-431-56535-2_3 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-5 year-2015"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/%E9%A0%86%E4%B8%80-%E5%B1%B1%E5%B2%B8/>順一 山岸</a></span>, <span><a href=/nii-yamagishilab/author/%E6%81%B5%E4%B8%80-%E5%BE%B3%E7%94%B0/>恵一 徳田</a></span>, <span><a href=/nii-yamagishilab/author/%E6%99%BA%E5%9F%BA-%E6%88%B8%E7%94%B0/>智基 戸田</a></span>, <span><a href=/nii-yamagishilab/author/%E3%82%88%E3%81%97%E3%81%93-%E3%81%BF%E3%82%8F/>よしこ みわ</a></span>, <span><a href=/nii-yamagishilab/author/%E5%9B%BD%E7%AB%8B%E6%83%85%E5%A0%B1%E5%AD%A6%E7%A0%94%E7%A9%B6%E6%89%80/>国立情報学研究所</a></span></span>
(2015).
<a href=/nii-yamagishilab/publication/bb-18335564/>おしゃべりなコンピュータ : 音声合成技術の現在と未来</a>.
<em>丸善出版</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ci.nii.ac.jp/ncid/BB18335564 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/bb-18335564/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2014"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=/nii-yamagishilab/author/tomi-kinnunen/>Tomi Kinnunen</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=/nii-yamagishilab/author/zhizheng-wu/>Zhizheng Wu</a></span>, <span><a href=/nii-yamagishilab/author/federico-alegre/>Federico Alegre</a></span>, <span><a href=/nii-yamagishilab/author/phillip-de-leon/>Phillip De Leon</a></span></span>
(2014).
<a href=/nii-yamagishilab/publication/evans-2014/>Speaker Recognition Anti-spoofing</a>.
<em>Handbook of Biometric Anti-Spoofing: Trusted Biometrics under Spoofing Attacks</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-1-4471-6524-8_7 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/evans-2014/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-1-4471-6524-8_7 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-6 year-2010"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=/nii-yamagishilab/author/sarah-creer/>Sarah Creer</a></span>, <span><a href=/nii-yamagishilab/author/phil-green/>Phil Green</a></span>, <span><a href=/nii-yamagishilab/author/stuart-cunningham/>Stuart Cunningham</a></span>, <span><a href=/nii-yamagishilab/author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2010).
<a href=/nii-yamagishilab/publication/creer-2010-building/>Building personalized synthetic voices for individuals with dysarthria using the HTS toolkit</a>.
<em>Computer Synthesized Speech Technologies: Tools for Aiding Impairment</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/nii-yamagishilab/publication/creer-2010-building/cite.bib>Cite</a></p></div></div></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Yamagishi Lab. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/nii-yamagishilab/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/nii-yamagishilab/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script>
<script src=/nii-yamagishilab/en/js/wowchemy.min.e8ee06ba8371980ffde659871dd593b0.js></script>
<script src=/nii-yamagishilab/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/nii-yamagishilab/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>