<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><meta name=author content="Junichi Yamagishi"><meta name=description content="Professor"><link rel=alternate hreflang=en-us href=https://zlin0.github.io/nii-yamagishilab/publication/><meta name=theme-color content="#3f51b5"><link rel=stylesheet href=../css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=../css/wowchemy.a59958c8a9dca96bf844ecb8c3aeb183.css><link rel=stylesheet href=../css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=../css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=alternate href=../publication/index.xml type=application/rss+xml title="NII Yamagishi's Lab"><link rel=manifest href=../manifest.webmanifest><link rel=icon type=image/png href=../media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=../media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://zlin0.github.io/nii-yamagishilab/publication/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@yamagishilab"><meta property="twitter:creator" content="@yamagishilab"><meta property="og:site_name" content="NII Yamagishi's Lab"><meta property="og:url" content="https://zlin0.github.io/nii-yamagishilab/publication/"><meta property="og:title" content="Publications | NII Yamagishi's Lab"><meta property="og:description" content="Professor"><meta property="og:image" content="https://zlin0.github.io/nii-yamagishilab/media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://zlin0.github.io/nii-yamagishilab/media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-01-01T00:00:00+00:00"><title>Publications | NII Yamagishi's Lab</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=3a079e7dad19be978a318345a7749d34><script src=../js/wowchemy-init.min.1ee5462d74c6c0de1f8881b384ecc58d.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=../>NII Yamagishi's Lab</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=../>NII Yamagishi's Lab</a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=../post><span>News</span></a></li><li class=nav-item><a class=nav-link href=../people><span>Members</span></a></li><li class=nav-item><a class=nav-link href=../publication><span>Publications</span></a></li><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>Materials</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=../materials/challenges/><span>Challenges</span></a>
<a class=dropdown-item href=../materials/downloads/><span>Downloads</span></a></div></li><li class=nav-item><a class=nav-link href=../contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Publications</h1></div><div class=universal-wrapper><div class=row><div class=col-lg-12><div class="form-row mb-4"><div class=col-auto><input type=search class="filter-search form-control form-control-sm" placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off role=textbox spellcheck=false></div><div class=col-auto><select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group=pubtype><option value=*>Type</option><option value=.pubtype-1>
Conference paper</option><option value=.pubtype-2>
Journal article</option><option value=.pubtype-3>
Preprint</option></select></div><div class=col-auto><select class="pub-filters form-control form-control-sm" data-filter-group=year><option value=*>Date</option><option value=.year-2022>
2022</option><option value=.year-2021>
2021</option></select></div></div><div id=container-publications><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/xuan-shi/>Xuan Shi</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/9670718/>Use of Speaker Recognition Approaches for Learning and Evaluating Embedding Representations of Musical Instrument Sounds</a>.
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9670718/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TASLP.2022.3140549 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/natalia-tomashenko/>Natalia Tomashenko</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/emmanuel-vincent/>Emmanuel Vincent</a></span>, <span><a href=../author/jose-patino/>Jose Patino</a></span>, <span><a href=../author/brij-mohan-lal-srivastava/>Brij Mohan Lal Srivastava</a></span>, <span><a href=../author/paul-gauthier-noe/>‪Paul-Gauthier Noé‬</a></span>, <span><a href=../author/andreas-nautsch/>Andreas Nautsch</a></span>, <span><a href=../author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/benjamin-obrien/>Benjamin O’Brien</a></span>, <span><a href=../author/others/>others</a></span></span>
(2022).
<a href=../publication/tomashenko-2022-voiceprivacy/>The VoicePrivacy 2020 Challenge: Results and findings</a>.
<em>Computer Speech & Language</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/tomashenko-2022-voiceprivacy/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1016/j.csl.2022.101362 target=_blank rel=noopener>Paper</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2109.00648 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.voiceprivacychallenge.org/ target=_blank rel=noopener>challenge website</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/wen-chin-huang/>Wen-Chin Huang</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/yu-tsao/>Yu Tsao</a></span>, <span><a href=../author/hsin-min-wang/>Hsin-Min Wang</a></span>, <span><a href=../author/tomoki-toda/>Tomoki Toda</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/huang-22-f-interspeech/>The VoiceMOS Challenge 2022</a>.
<em>Proc. Interspeech 2022</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/huang-22-f-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2022-970 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.11389 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://codalab.lisn.upsaclay.fr/competitions/695 target=_blank rel=noopener>CodaLab</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://voicemos-challenge-2022.github.io/ target=_blank rel=noopener>website</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/lin-zhang/>Lin Zhang</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/zhang-2022-partialspoof/>The PartialSpoof Database and Countermeasures for the Detection of Short Generated Audio Segments Embedded in a Speech Utterance</a>.
<em>arXiv preprint arXiv:2204.05177</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-2022-partialspoof/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2204.05177 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/cheng-hung-hu/>Cheng-Hung Hu</a></span>, <span><a href=../author/yu-huai-peng/>Yu-Huai Peng</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/yu-tsao/>Yu Tsao</a></span>, <span><a href=../author/hsin-min-wang/>Hsin-Min Wang</a></span></span>
(2022).
<a href=../publication/9716822/>SVSNet: An End-to-End Speaker Voice Similarity Assessment Model</a>.
<em>IEEE Signal Processing Letters</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9716822/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/LSP.2022.3152672 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2107.09392 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/chang-zeng/>Chang Zeng</a></span>, <span><a href=../author/lin-zhang/>Lin Zhang</a></span>, <span><a href=../author/meng-liu/>Meng Liu</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/zeng-22-interspeech/>Spoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022</a>.
<em>Proc. Interspeech 2022</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zeng-22-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2022-10495 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2209.00423 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/wang-2022-spoofed/>Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders</a>.
<em>arXiv preprint arXiv:2210.10570</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-2022-spoofed/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2210.10570 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/li-kuang-chen/>Li-Kuang Chen</a></span>, <span><a href=../author/canasai-kruengkrai/>Canasai Kruengkrai</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/chen-2022-outlier/>Outlier-Aware Training for Improving Group Accuracy Disparities</a>.
<em>arXiv preprint arXiv:2210.15183</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chen-2022-outlier/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2210.15183 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/jtt-m target=_blank rel=noopener>Codes</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.7260028 target=_blank rel=noopener>Pre-trained models</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/anssi-kanervisto/>Anssi Kanervisto</a></span>, <span><a href=../author/ville-hautam#x00e4ki/>Ville Hautam&amp;#x00E4;ki</a></span>, <span><a href=../author/tomi-kinnunen/>Tomi Kinnunen</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/9664367/>Optimizing Tandem Speaker Verification and Anti-Spoofing Systems</a>.
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9664367/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TASLP.2021.3138681 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/cheng-i-jeff-lai/>Cheng-I Jeff Lai</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/yang-zhang/>Yang Zhang</a></span>, <span><a href=../author/shiyu-chang/>Shiyu Chang</a></span>, <span><a href=../author/kaizhi-qian/>Kaizhi Qian</a></span>, <span><a href=../author/yi-lun-liao/>Yi-Lun Liao</a></span>, <span><a href=../author/yung-sung-chuang/>Yung-Sung Chuang</a></span>, <span><a href=../author/alexander-h.-liu/>Alexander H. Liu</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/david-cox/>David Cox</a></span>, <span><a href=../author/james-glass/>James Glass</a></span></span>
(2022).
<a href=../publication/9747728/>On the Interplay between Sparsity, Naturalness, Intelligibility, and Prosody in Speech Synthesis</a>.
<em>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9747728/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP43922.2022.9747728 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.01147 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://people.csail.mit.edu/clai24/prune-tts/ target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/canasai-kruengkrai/>Canasai Kruengkrai</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/kruengkrai-2022-mitigating/>Mitigating the Diminishing Effect of Elastic Weight Consolidation</a>.
<em>Proceedings of the 29th International Conference on Computational Linguistics</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kruengkrai-2022-mitigating/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.coling-1.403.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/ewc target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/huy-hong-nguyen/>Huy Hong Nguyen</a></span>, <span><a href=../author/sebastien-marcel/>Sebastien Marcel</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/isao-echizen/>Isao Echizen</a></span></span>
(2022).
<a href=../publication/9758063/>Master Face Attacks on Face Recognition Systems</a>.
<em>IEEE Transactions on Biometrics, Behavior, and Identity Science</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9758063/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TBIOM.2022.3166206 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/natalia-tomashenko/>Natalia Tomashenko</a></span></span>
(2022).
<a href=../publication/miao-22-odyssey/>Language-Independent Speaker Anonymization Approach Using Self-Supervised Pre-Trained Models</a>.
<em>Proc. The Speaker and Language Recognition Workshop (Odyssey 2022)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/miao-22-odyssey/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Odyssey.2022-39 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2202.13097 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/SAS-audio-samples/ target=_blank rel=noopener>Samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/SSL-SAS target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/chang-zeng/>Chang Zeng</a></span>, <span><a href=../author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/zeng-2022-joint/>Joint Speaker Encoder and Neural Back-end Model for Fully End-to-End Automatic Speaker Verification with Multiple Enrollment Utterances</a>.
<em>arXiv preprint arXiv:2209.00485</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zeng-2022-joint/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2209.00485 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/haoyu-li/>Haoyu Li</a></span>, <span><a href=../author/yun-liu/>Yun Liu</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/li-2022-joint/>Joint Noise Reduction and Listening Enhancement for Full-End Speech Enhancement</a>.
<em>arXiv preprint arXiv:2203.11500</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/li-2022-joint/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.11500 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/wang-22-odyssey/>Investigating Self-Supervised Front Ends for Speech Spoofing Countermeasures</a>.
<em>Proc. The Speaker and Language Recognition Workshop (Odyssey 2022)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-22-odyssey/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Odyssey.2022-14 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2111.07725 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junich-yamagishi/>Junich Yamagishi</a></span></span>
(2022).
<a href=../publication/wang-2022-investigating/>Investigating Active-learning-based Training Data Selection for Speech Spoofing Countermeasure</a>.
<em>arXiv preprint arXiv:2203.14553</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-2022-investigating/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.14553 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/paul-gauthier-noe/>‪Paul-Gauthier Noé‬</a></span>, <span><a href=../author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/jean-francois-bonastre/>Jean-François Bonastre</a></span>, <span><a href=../author/driss-matrouf/>Driss Matrouf</a></span></span>
(2022).
<a href=../publication/noe-2022-hiding/>Hiding speaker's sex in speech using zero-evidence speaker representation in an analysis/synthesis pipeline</a>.
<em>arXiv preprint arXiv:2211.16065</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/noe-2022-hiding/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2211.16065 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/speaker_sex_attribute_privacy target=_blank rel=noopener>Codes and samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://zenodo.org/record/7347685 target=_blank rel=noopener>Pre-trained models</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/wen-chin-huang/>Wen-Chin Huang</a></span>, <span><a href=../author/tomoki-toda/>Tomoki Toda</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/9746395/>Generalization Ability of MOS Prediction Networks</a>.
<em>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9746395/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP43922.2022.9746395 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.02635 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/mos-finetune-ssl target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/9746204/>Estimating the Confidence of Speech Spoofing Countermeasure</a>.
<em>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9746204/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP43922.2022.9746204 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.04775 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/haoyu-li/>Haoyu Li</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/li-22-e-interspeech/>DDS: A new device-degraded speech dataset for speech enhancement</a>.
<em>Proc. Interspeech 2022</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/li-22-e-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2022-441 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2109.07931 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.5464104 target=_blank rel=noopener>Database DDS database (DAPS portion)</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.5499506 target=_blank rel=noopener>DDS database (VCTK portion part 1)</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.5501697 target=_blank rel=noopener>DDS database (VCTK portion part 2)</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/xuan-shi/>Xuan Shi</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/shrikanth-narayanan/>Shrikanth Narayanan</a></span></span>
(2022).
<a href=../publication/shi-2022-can/>Can Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems?</a>.
<em>arXiv preprint arXiv:2211.13868</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/shi-2022-can/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=hhttps://arxiv.org/abs/2211.13868>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/sample-midi-to-audio/ target=_blank rel=noopener>Samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://zenodo.org/record/7370009 target=_blank rel=noopener>Pre-trained models</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/hemlata-tak/>Hemlata Tak</a></span>, <span><a href=../author/massimiliano-todisco/>Massimiliano Todisco</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/jee-weon-jung/>Jee-weon Jung</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/nicholas-evans/>Nicholas Evans</a></span></span>
(2022).
<a href=../publication/tak-22-odyssey/>Automatic Speaker Verification Spoofing and Deepfake Detection Using Wav2vec 2.0 and Data Augmentation</a>.
<em>Proc. The Speaker and Language Recognition Workshop (Odyssey 2022)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/tak-22-odyssey/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Odyssey.2022-16 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2202.12233 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/chang-zeng/>Chang Zeng</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2022).
<a href=../publication/9746688/>Attention Back-End for Automatic Speaker Verification with Multiple Enrollment Utterances</a>.
<em>ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9746688/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP43922.2022.9746688 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2104.01541v2 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/Attention_Backend_for_ASV target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/xiaoxiao-miao/>XiaoXiao Miao</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/natalia-tomashenko/>Natalia Tomashenko</a></span></span>
(2022).
<a href=../publication/miao-22-interspeech/>Analyzing Language-Independent Speaker Anonymization Framework under Unseen Conditions</a>.
<em>Proc. Interspeech 2022</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/miao-22-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2022-11065 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.14834 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/SAS-audio-samples/ target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/cooper-21-b-ssw/>Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis</a>.
<em>Proc. 11th ISCA Speech Synthesis Workshop (SSW 11)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/cooper-21-b-ssw/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SSW.2021-23 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2104.12292 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/samples-xin/main-midi2audio.html target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/jennifer-williams/>Jennifer Williams</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/paul-gauthier/>Paul-Gauthier</a></span>, <span><a href=../author/cassia-valentini-botinhao/>Cassia Valentini-Botinhao</a></span>, <span><a href=../author/jean-francois-bonastre/>Jean-François Bonastre</a></span></span>
(2021).
<a href=../publication/williams-21-spsc/>Revisiting Speech Content Privacy</a>.
<em>Proc. 2021 ISCA Symposium on Security and Privacy in Speech Communication</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/williams-21-spsc/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SPSC.2021-9 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2110.06760 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/trung-nghia-le/>Trung-Nghia Le</a></span>, <span><a href=../author/huy-hong-nguyen/>Huy Hong Nguyen</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/isao-echizen/>Isao Echizen</a></span></span>
(2021).
<a href=../publication/9711250/>OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild</a>.
<em>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9711250/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICCV48922.2021.00996 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2107.14480 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.5528418 target=_blank rel=noopener>Database</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/lin-zhang/>Lin Zhang</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/zhang-21-asvspoof/>Multi-task Learning in Utterance-level and Segmental-level Spoof Detection</a>.
<em>Proc. 2021 Edition of the Automatic Speaker Verification and Spoofing Countermeasures Challenge</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-21-asvspoof/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/ASVSPOOF.2021-2 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2107.14132 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://zenodo.org/record/5112031 target=_blank rel=noopener>Database</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/haoyu-li/>Haoyu Li</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/9536406/>Multi-Metric Optimization Using Generative Adversarial Networks for Near-End Speech Intelligibility Enhancement</a>.
<em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9536406/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TASLP.2021.3111566 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2104.08499 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/hyli666-demos/intelligibility/index.html target=_blank rel=noopener>Samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/NELE-GAN target=_blank rel=noopener>Codes</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/jennifer-williams/>Jennifer Williams</a></span>, <span><a href=../author/yi-zhao/>Yi Zhao</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/9413543/>Learning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm</a>.
<em>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9413543/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP39728.2021.9413543 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2010.10727 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://rhoposit.github.io/icassp2021 target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/shuhei-kato/>Shuhei Kato</a></span>, <span><a href=../author/yusuke-yasuda/>Yusuke Yasuda</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/9414175/>How Similar or Different is Rakugo Speech Synthesizer to Professional Performers?</a>.
<em>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9414175/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP39728.2021.9414175 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2010.11549 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/cooper-21-ssw/>How do Voices from Past Speech Synthesis Challenges Compare Today?</a>.
<em>Proc. 11th ISCA Speech Synthesis Workshop (SSW 11)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/cooper-21-ssw/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SSW.2021-32 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2105.02373 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/jennifer-williams/>Jennifer Williams</a></span>, <span><a href=../author/jason-fong/>Jason Fong</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/williams-21-ssw/>Exploring Disentanglement with Multilingual and Monolingual VQ-VAE</a>.
<em>Proc. 11th ISCA Speech Synthesis Workshop (SSW 11)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/williams-21-ssw/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SSW.2021-22 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2105.01573 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://rhoposit.github.io/ssw11 target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/haoyu-li/>Haoyu Li</a></span>, <span><a href=../author/yang-ai/>Yang Ai</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/9383507/>Enhancing Low-Quality Voice Recordings Using Disentangled Channel Factor and Neural Waveform Model</a>.
<em>2021 IEEE Spoken Language Technology Workshop (SLT)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9383507/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/SLT48900.2021.9383507 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2011.05038 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/hyli666-demos/evr-slt2021 target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/yusuke-yasuda/>Yusuke Yasuda</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishd/>Junichi Yamagishd</a></span></span>
(2021).
<a href=../publication/9414499/>End-to-End Text-to-Speech Using Latent Duration Based on VQ-VAE</a>.
<em>ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9414499/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICASSP39728.2021.9414499 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2010.09602 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/sample-tts-latent-duration/ target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/khanh-duv-nguyen/>Khanh-Duv Nguyen</a></span>, <span><a href=../author/huv-h.-nguyen/>Huv H. Nguyen</a></span>, <span><a href=../author/trung-nghia-le/>Trung-Nghia Le</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/isao-echizen/>Isao Echizen</a></span></span>
(2021).
<a href=../publication/9667046/>Effectiveness of Detection-based and Regression-based Approaches for Estimating Mask-Wearing Ratio</a>.
<em>2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9667046/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/FG52635.2021.9667046 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2111.12888 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/yang-ai/>Yang Ai</a></span>, <span><a href=../author/haoyu-li/>Haoyu Li</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/zhenhua-ling/>Zhenhua Ling</a></span></span>
(2021).
<a href=../publication/9383611/>Denoising-and-Dereverberation Hierarchical Neural Vocoder for Robust Waveform Generation</a>.
<em>2021 IEEE Spoken Language Technology Workshop (SLT)</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9383611/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/SLT48900.2021.9383611 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2011.03955 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=http://home.ustc.edu.cn/~ay8067/DNR/demo.html target=_blank rel=noopener>Samples</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/jean-francois-bonastre/>Jean-François Bonastre</a></span>, <span><a href=../author/hector-delgado/>Héctor Delgado</a></span>, <span><a href=../author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=../author/tomi-kinnunen/>Tomi Kinnunen</a></span>, <span><a href=../author/kong-aik-lee/>Kong Aik Lee</a></span>, <span><a href=../author/xuechen-liu/>Xuechen Liu</a></span>, <span><a href=../author/andreas-nautsch/>Andreas Nautsch</a></span>, <span><a href=../author/paul-gauthier-noe/>‪Paul-Gauthier Noé‬</a></span>, <span><a href=../author/jose-patino/>Jose Patino</a></span>, <span><a href=../author/md-sahidullah/>Md Sahidullah</a></span>, <span><a href=../author/brij-mohan-lal-srivastava/>Brij Mohan Lal Srivastava</a></span>, <span><a href=../author/massimiliano-todisco/>Massimiliano Todisco</a></span>, <span><a href=../author/natalia-tomashenko/>Natalia Tomashenko</a></span>, <span><a href=../author/emmanuel-vincent/>Emmanuel Vincent</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/bonastre-21-spsc/>Benchmarking and challenges in security and privacy for voice biometrics</a>.
<em>Proc. 2021 ISCA Symposium on Security and Privacy in Speech Communication</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/bonastre-21-spsc/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/SPSC.2021-11 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2109.00281 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/massimiliano-todisco/>Massimiliano Todisco</a></span>, <span><a href=../author/md-sahidullah/>Md Sahidullah</a></span>, <span><a href=../author/jose-patino/>Jose Patino</a></span>, <span><a href=../author/andreas-nautsch/>Andreas Nautsch</a></span>, <span><a href=../author/xuechen-liu/>Xuechen Liu</a></span>, <span><a href=../author/kong-aik-lee/>Kong Aik Lee</a></span>, <span><a href=../author/tomi-kinnunen/>Tomi Kinnunen</a></span>, <span><a href=../author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=../author/hector-delgado/>Héctor Delgado</a></span></span>
(2021).
<a href=../publication/yamagishi-21-asvspoof/>ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection</a>.
<em>Proc. 2021 Edition of the Automatic Speaker Verification and Spoofing Countermeasures Challenge</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/yamagishi-21-asvspoof/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/ASVSPOOF.2021-8 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2109.00537 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.asvspoof.org/ target=_blank rel=noopener>challenge website</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/andreas-nautsch/>Andreas Nautsch</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/nicholas-evans/>Nicholas Evans</a></span>, <span><a href=../author/tomi-h.-kinnunen/>Tomi H. Kinnunen</a></span>, <span><a href=../author/ville-vestman/>Ville Vestman</a></span>, <span><a href=../author/massimiliano-todisco/>Massimiliano Todisco</a></span>, <span><a href=../author/hector-delgado/>Héctor Delgado</a></span>, <span><a href=../author/md-sahidullah/>Md Sahidullah</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/kong-aik-lee/>Kong Aik Lee</a></span></span>
(2021).
<a href=../publication/9358099/>ASVspoof 2019: Spoofing Countermeasures for the Detection of Synthesized, Converted and Replayed Speech</a>.
<em>IEEE Transactions on Biometrics, Behavior, and Identity Science</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/9358099/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TBIOM.2021.3059479 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2102.05889 target=_blank rel=noopener>Preprint</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/lin-zhang/>Lin Zhang</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/erica-cooper/>Erica Cooper</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/jose-patino/>Jose Patino</a></span>, <span><a href=../author/nicholas-evans/>Nicholas Evans</a></span></span>
(2021).
<a href=../publication/zhang-21-ca-interspeech/>An Initial Investigation for Detecting Partially Spoofed Audio</a>.
<em>Proc. Interspeech 2021</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-21-ca-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2021-738 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2104.02518 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://nii-yamagishilab.github.io/zlin-demo/IS2021/index.html target=_blank rel=noopener>Samples</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.5281/zenodo.4817532 target=_blank rel=noopener>Database</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/canasai-kruengkrai/>Canasai Kruengkrai</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span>, <span><a href=../author/xin-wang/>Xin Wang</a></span></span>
(2021).
<a href=../publication/kruengkrai-2021-multi/>A multi-level attention model for evidence-based fact checking</a>.
<em>arXiv preprint arXiv:2106.00950</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/kruengkrai-2021-multi/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2106.00950 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/mla target=_blank rel=noopener>Code</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span><a href=../author/xin-wang/>Xin Wang</a></span>, <span><a href=../author/junichi-yamagishi/>Junichi Yamagishi</a></span></span>
(2021).
<a href=../publication/wang-21-fa-interspeech/>A Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection</a>.
<em>Proc. Interspeech 2021</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-21-fa-interspeech/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.21437/Interspeech.2021-702 target=_blank rel=noopener>DOI</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2103.11326 target=_blank rel=noopener>Preprint</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts target=_blank rel=noopener>Codes</a></p></div></div></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2022 Yamagishi Lab. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=../js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=../js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=../en/js/wowchemy.min.54dd6e4d8f2e4b1d098381b57f18dd83.js></script>
<script src=../js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=../js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>