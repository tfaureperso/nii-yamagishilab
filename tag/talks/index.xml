<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>talks | NII Yamagishi's Lab</title><link>https://zlin0.github.io/nii-yamagishilab/tag/talks/</link><atom:link href="https://zlin0.github.io/nii-yamagishilab/tag/talks/index.xml" rel="self" type="application/rss+xml"/><description>talks</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 20 Dec 2022 00:00:00 +0000</lastBuildDate><image><url>https://zlin0.github.io/nii-yamagishilab/media/icon_hu7af45c05e7eaab656aeab8d5fe153c59_37125_512x512_fill_lanczos_center_3.png</url><title>talks</title><link>https://zlin0.github.io/nii-yamagishilab/tag/talks/</link></image><item><title>[Dec. 21] Predicting and synthesising plausible speech examples after oral cancer treatment</title><link>https://zlin0.github.io/nii-yamagishilab/talks/2022-12-21-bh/</link><pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talks/2022-12-21-bh/</guid><description>&lt;p>Pathological speech synthesis has traditionally been focused on improving the error rates of downstream pathological automatic speech recognition (ASR) tasks. However, this field has the potential to have a much broader impact beyond just pathological ASR. A speech synthesis system that could predict how a patient&amp;rsquo;s voice would sound after tongue surgery or the onset of a disease, such as dysarthria, could be very useful for clinicians and patients. Such a system could be based on a surgical plan or metadata about the patient, and it could help clinicians make more informed decisions about the surgery. It could also help alleviate the stress of patients by giving them a better idea of what to expect after the surgery or onset of the disease. The talk will discuss the current progress and challenges in synthesising and evaluating pathological speech, both using voice conversion and articulatory synthesis methods.&lt;/p></description></item><item><title>[Dec. 20] Textless Phrase-Structure Induction from Visually-Grounded Speech</title><link>https://zlin0.github.io/nii-yamagishilab/talks/2022-12-20-jeff/</link><pubDate>Mon, 19 Dec 2022 00:00:00 +0000</pubDate><guid>https://zlin0.github.io/nii-yamagishilab/talks/2022-12-20-jeff/</guid><description>&lt;p>We study phrase structure induction from visually-grounded speech without intermediate text or text pre-trained models. The core idea is to first segment the speech waveform into sequences of word segments, then induce phrase structure based on the inferred segment-level continuous representations. To this end, we present the Audio-Visual Neural Syntax Learner (AV-NSL) that learns non-trivial phrase structure by listening to audio and looking at images, without ever reading text. Experiments on SpokenCOCO, the spoken version of MSCOCO with paired images and spoken captions, show that AV-NSL infers meaningful phrase structures similar to those learned from naturally-supervised text parsing, quantitatively and qualitatively. The findings in this paper extend prior work in unsupervised language acquisition from speech and grounded grammar induction, and manifest one possibility of bridging the gap between the two fields.&lt;/p></description></item></channel></rss>